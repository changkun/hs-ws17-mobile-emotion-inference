\section{Applications Case Study}\label{sec:applications}

Assuming that user emotions has inferred by models or systems, \cite{Conati2005} address a variety of issues related to the development of affective loop as well as synthesis of the appropriate affective expressions. In subsequent researches emotional systems was sugussted in \cite{Crane2007} since emotional-sensitive system make our interactions with machines like human to human communications\cite{picard1999affective}.
expressions. In this section, we illustrate the most popular applications of emotion-sensitive HCI system in the mobile affective research area.

\subsection{Case 1: Spoken Dialogue Systems}

A voice interaction system capable of sending the users emotional messages is able to improving the intelligence and interaction experience of a voice user interface \cite{surace2001voice}.

In the early research stage, \cite{kostov2000emotion} proposed a personalized voice-emotion user interface in desktop system regardless of speaker's age, sex or language is presented. They experimented with participants and the results showed that voice emotion sensitive agents are feasible.

The most recent papers in emotion-sensitive voice user interface considers emotional voice tones to caring users \cite{chang2016intelligent}, as well as the voice control system in an in-vehicle infotainment system \cite{kim2016effects}.

Due to the insivible property of voice interaction system, it is almost endless of how we integrate user emotions to a spoken dialogue system, \cite{mctear2016rise} addressed emotional spoken system by a markup language, which means developers can easily integrate user emotion state to adjust the system voice. 

With the raising of voice assistent, there already exists successful commercial voice system such as Apple Siri\footnote{\url{https://www.apple.com/ios/siri/}}, Google Assistant\footnote{\url{https://assistant.google.com/}}, Amazon Alexa\footnote{\url{https://developer.amazon.com/alexa}} and Microsoft Cortana\footnote{\url{https://www.microsoft.com/en-us/windows/cortana}}. Voice user interfaces design become on board of user experience design. Even thgough they provides this kind of markup language, there still a huge unmined research to an open problem of how to evaluate this kind of emotional voice system.

\subsection{Case 2: Adaptive Graphics User Interfaces}
Adaptive user interfaces has been researched for years \cite{schneider1993adaptive} and addressed in many ways. User emotion is one of the aspect of adaptive user interface.

Dalvand et al.\cite{Dalvand} introduced an adaptive user interface, the colors of user interface change according to the emotional states and mood state of users. Emotional states of a user are specified according to his/her interactions with the keyboard. After detection of emotions, the user mood reflects appropriate colors, and \cite{Kaiser2006} as another example of this adaption. However they didn't have a good evaluation of such kind of system.
A recent paper \cite{Galindo} also addresses UI adaptation by user emotions (positive, negative and neutral) at run-time. Their prototype was tested successfully of how it reacts to emotions (negative).

In conclusion, the adaptive graphics user interfaces system suguessts covered components with inferring engine (integratable with techniques we discussed in previous section), the adaptation engine (with typical personalized system rules) and the interactive system (for normal graphics user interfaces). It also evidenced that GUI changes denotes emotion changes in run-time, which was particularly beneficial for most users.


