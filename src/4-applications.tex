\section{Applications Case Study}\label{sec:applications}

Assuming that user emotions have inferred by models or systems, Conati et al.~\cite{Conati2005} address a variety of issues related to the development of affective loop as well as a synthesis of the appropriate affective expressions. In subsequent studies, emotional systems were suggested in~\cite{Crane2007} since emotional-sensitive systems make our interactions with machines like human to human communications~\cite{picard1999affective}. In this section, we illustrate the most popular applications of emotion-sensitive HCI system in the mobile affective research area.

\subsection{Case 1: Spoken Dialogue Systems}

A voice interaction system capable of sending the users emotional messages can improve the intelligence and interaction experience of a voice user interface \cite{surace2001voice}.

In the early research stage, \cite{kostov2000emotion} proposed a personalized voice-emotion user interface in the desktop system regardless of speaker's age, sex or language is presented. They experimented with participants, and the results showed that voice emotion sensitive agents are feasible.

The most recent papers in emotion-sensitive voice user interface consider emotional voice tones to caring users~\cite{chang2016intelligent}, as well as the voice control system in an in-vehicle infotainment system~\cite{kim2016effects}.

Due to the intangible property of voice interaction system, it is almost endless of how we integrate user emotions to a spoken dialogue system, McTear et al.~\cite{mctear2016rise} addressed emotional spoken system by a markup language, which means developers can easily incorporate user emotion state to adjust the system voice. 

With the raising of voice assistant, there already exists successful commercial voice system such as Apple Siri\footnote{\url{https://www.apple.com/ios/siri/}}, Google Assistant\footnote{\url{https://assistant.google.com/}}, Amazon Alexa\footnote{\url{https://developer.amazon.com/alexa}} and Microsoft Cortana\footnote{\url{https://www.microsoft.com/en-us/windows/cortana}}. 
Voice user interfaces design has become the part of user experience design. Even though the commercial system provides the above described markup language, there is still huge unmined research into an open problem of how to evaluate this kind of emotional voice system.

\subsection{Case 2: Adaptive Graphics User Interfaces}
Adaptive user interfaces have been researched for years \cite{schneider1993adaptive, langley1997machine} and addressed in many ways. User emotion is one of the aspects that considered in adaptive user interface.

Dalvand et al.~\cite{Dalvand} introduced an adaptive user interface, the colors of user interface change according to the emotional states and mood state of users. Emotional states of a user are specified according to his/her interactions with the keyboard. After detection of emotions, the user mood reflects appropriate colors, and \cite{Kaiser2006} as another example of this adaption. However, they even didn't have an evaluation of such kind of system.
A recent paper~\cite{Galindo} also addresses UI adaptation by user emotions (positive, negative and neutral) at run-time. Their prototype was tested successfully of how it reacts to emotions (negative).

In conclusion, the adaptive graphics user interfaces system suggests covered components with inference engine (integrate with techniques we discussed in the previous section), the adaptation engine (with common personalized system rules) and the interactive system (for standard graphical user interfaces). It also evidenced that GUI changes follows emotion changes at run-time, which was mainly beneficial for most users.
