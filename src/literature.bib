@article{Garcia-Garcia2017,
abstract = {Emotion detection has become one of the most important aspects to consider in any project related to Affective Computing. Due to the almost endless applications of this new discipline, the development of emotion detection technologies has brought up as a quite profitable opportunity in the corporate sector. Many start-up enterprises have emerged in the last years, dedicated almost exclusively to a specific type of emotion detection technology. In this paper, we present a thorough review of current technologies to detect human emotions. To this end, we explore the different sources from which emotions can be read, along with existing technologies developed to recognize them. We also explore some application domains in which this technology has been applied. This survey has let us identify the strengths and shortcomings of current technology for emotion detection. We conclude the survey highlighting the aspects that requires further research and development.},
author = {Garcia-Garcia, Jose Maria and Penichet, Victor M R and Lozano, Maria D},
doi = {10.1145/3123818.3123852},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Garcia-Garcia, Penichet, Lozano/Interacci{\'{o}}n 2017/Garcia-Garcia, Penichet, Lozano - 2017 - Emotion Detection A Technology review 1.pdf:pdf},
isbn = {9781450352291},
journal = {Interacci{\'{o}}n 2017},
keywords = {Affective Computing,emotion recognition,technologies},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Emotion Detection: A Technology review 1}},
year = {2017}
}
@article{Conati2005,
author = {Conati, Cristina and Marsella, Stacy and Paiva, Ana},
doi = {10.1145/1040830.1040838},
file = {:Users/changkun/Documents/Mendeley Desktop/2005/Conati, Marsella, Paiva/Proceedings of the 10th international conference on Intelligent user interfaces - IUI '05/Conati, Marsella, Paiva - 2005 - Affective interactions.pdf:pdf},
isbn = {1581138946},
journal = {Proc. 10th Int. Conf. Intell. user interfaces  - IUI '05},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {7},
title = {{Affective interactions}},
url = {http://portal.acm.org/citation.cfm?doid=1040830.1040838},
year = {2005}
}
@article{Kaiser2006,
author = {Kaiser, Robin and Oertel, Karina},
file = {:Users/changkun/Documents/Mendeley Desktop/2006/Kaiser, Oertel/Computer/Kaiser, Oertel - 2006 - Emotions in HCI – An Affective E-Learning System.pdf:pdf},
journal = {Computer (Long. Beach. Calif).},
keywords = {affective computing,are sensors for e,e-learning,evaluated and enhanced,g,hci,integrated in the glove,resistance and skin conductivity,sensor data are wirelessly,skin,transmitted and made available},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {105--106},
title = {{Emotions in HCI – An Affective E-Learning System}},
year = {2006}
}
@article{Zhang2014,
abstract = {This survey presents recent progress on Affective Computing (AC) using mobile devices. AC has been one of the most active research topics for decades. The primary limitation of traditional AC research refers to as impermeable emotions. This criticism is prominent when emotions are investigated outside social contexts. It is problematic because some emotions are directed at other people and arise from interactions with them. The development of smart mobile wearable devices (e.g., Apple Watch, Google Glass, iPhone, Fitbit) enables the wild and natural study for AC in the aspect of computer science. This survey emphasizes the AC study and system using smart wearable devices. Various models, methodologies and systems are discussed in order to examine the state of the art. Finally, we discuss remaining challenges and future works.},
archivePrefix = {arXiv},
arxivId = {1410.1648},
author = {Zhang, Shengkai and Hui, Pan},
eprint = {1410.1648},
file = {:Users/changkun/Documents/Mendeley Desktop/2014/Zhang, Hui/Unknown/Zhang, Hui - 2014 - A Survey on Mobile Affective Computing.pdf:pdf},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
title = {{A Survey on Mobile Affective Computing}},
url = {http://arxiv.org/abs/1410.1648},
year = {2014}
}
@book{Hutchison2011,
abstract = {Interest in emotion detection is increasing significantly. For research and development in the field of Affective Computing and emotion-aware interaction techniques, reliable and robust technology is needed for detecting emotional signs in users under everyday conditions. In this paper, a novel wearable system for measuring emotion-related physiological parameters is presented. Currently heart rate, skin conductivity, and skin temperature are taken; further sensors can easily be added. The system is very easy to use, robust, and suitable for mobile and long-time logging of data. It has an open architecture and can easily be integrated into other systems or applications. The system is designed for use in emotion research as well as in everyday affective applications.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Hutchison, David and Mitchell, John C},
doi = {10.1007/978-3-642-24600-5},
eprint = {9780201398298},
file = {:Users/changkun/Documents/Mendeley Desktop/2011/Hutchison, Mitchell/Unknown/Hutchison, Mitchell - 2011 - Affective Computing and Intelligent Interaction.pdf:pdf},
isbn = {978-3-642-24599-2},
issn = {03029743},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pmid = {4520227},
title = {{Affective Computing and Intelligent Interaction}},
url = {http://link.springer.com/10.1007/978-3-642-24600-5},
volume = {6974},
year = {2011}
}
@article{Bailenson2007,
abstract = {This article examines the phenomenon of Virtual Interpersonal Touch (VIT), people touching one another via force-feedback haptic devices. As collaborative virtual environments become utilized more effectively, it is only natural that interactants will have the ability to touch one another. In the work presented here, we used relatively basic devices to begin to explore the expression of emotion through VIT. In Experiment 1, participants utilizeda2DOF force-feedback joy- stick to express seven emotions.We examined various dimensions of the forces generated and subjective ratings of the difficulty of expressing those emotions. In Experiment 2, a separate group of participants attempted to recognize the recordings of emotions generated in Experiment 1. In Experiment 3, pairs of partici- pants attempted to communicate the seven emotions using physical handshakes. Results indicated that humans were above chance when recognizing emotions via VIT but not as accurate as people expressing emotions through nonmediated handshakes.We discuss a theoretical framework for understanding emotions ex- pressed through touch as well as the implications of the current findings for the utilization of VIT in human–computer interaction.},
author = {Bailenson, Jeremy N and Yee, Nick and Brave, Scott and Merget, Dan and Koslow, David},
doi = {10.1080/07370020701493509},
file = {:Users/changkun/Documents/Mendeley Desktop/2007/Bailenson et al/Human-Computer Interaction/Bailenson et al. - 2007 - Virtual interpersonal touch expressing and recognizing emotions through haptic devices.pdf:pdf},
isbn = {0737-0024},
issn = {0737-0024},
journal = {Human-Computer Interact.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {November},
pages = {325--353},
title = {{Virtual interpersonal touch: expressing and recognizing emotions through haptic devices}},
volume = {22},
year = {2007}
}
@article{Mottelson2016,
abstract = {Current techniques to computationally detect human affect often depend on specialized hardware, work only in laboratory settings, or require substantial individual training. We use sen- sors in commodity smartphones to estimate affect in the wild with no training time based on a link between affect and move- ment. The first experiment had 55 participants do touch inter- actions after exposure to positive or neutral emotion-eliciting films; negative affect resulted in faster but less precise inter- actions, in addition to differences in rotation and acceleration. Using off-the-shelf machine learning algorithms we report 89.1{\%} accuracy in binary affective classification, grouping participants by their self-assessments. A follow up experiment validated findings from the first experiment; the experiment collected naturally occurring affect of 127 participants, who again did touch interactions. Results demonstrate that affect has direct behavioral effect on mobile interaction and that affect detection using common smartphone sensors is feasible.},
author = {Mottelson, Aske and Hornb{\ae}k, Kasper},
doi = {10.1145/2971648.2971654},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Mottelson, Hornb{\ae}k/ACM International Joint Conference on Pervasive and Ubiquitous Computing/Mottelson, Hornb{\ae}k - 2016 - An affect detection technique using mobile commodity sensors in the wild.pdf:pdf},
isbn = {1450344615},
journal = {ACM Int. Jt. Conf. Pervasive Ubiquitous Comput.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {781--792},
title = {{An affect detection technique using mobile commodity sensors in the wild}},
year = {2016}
}
@article{Hertenstein2009,
abstract = {The study of emotional communication has focused predominantly on the facial and vocal channels but has ignored the tactile channel. Participants in the current study were allowed to touch an unacquainted partner on the whole body to communicate distinct emotions. Of interest was how accurately the person being touched decoded the intended emotions without seeing the tactile stimulation. The data indicated that anger, fear, disgust, love, gratitude, and sympathy were decoded at greater than chance levels, as well as happiness and sadness, 2 emotions that have not been shown to be communicated by touch to date. Moreover, fine-grained coding documented specific touch behaviors associated with different emotions. The findings are discussed in terms of their contribution to the study of emotion-related communication.},
author = {Hertenstein, Matthew J and Hertenstein, Matthew J and Holmes, Rachel and Holmes, Rachel and Mccullough, Margaret and Mccullough, Margaret and Keltner, Dacher and Keltner, Dacher},
doi = {10.1037/a0016108},
file = {:Users/changkun/Documents/Mendeley Desktop/2009/Hertenstein et al/Emotion/Hertenstein et al. - 2009 - The Communication of Emotion via Touch.pdf:pdf},
issn = {1528-3542},
journal = {Emotion},
keywords = {communication,emotion,tactile,touch},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {4},
pages = {566 --573},
pmid = {19653781},
title = {{The Communication of Emotion via Touch}},
volume = {9},
year = {2009}
}
@article{Rana,
author = {Rana, Rajib and Hume, Margee},
file = {:Users/changkun/Documents/Mendeley Desktop/Unknown/Rana, Hume/Unknown/Rana, Hume - Unknown - Opportunistic and Context - aware Affect Sensing on Smartphones The Concept , Challenges and Opportunities .pdf:pdf},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {1--13},
title = {{Opportunistic and Context - aware Affect Sensing on Smartphones : The Concept , Challenges and Opportunities .}}
}
@article{Politou2017,
abstract = {The spontaneous recognition of emotional states and personality traits of individuals has been puzzling researchers for years whereas pertinent studies demonstrating the progress in the field, despite their diversity, are still encouraging. This work surveys the most well-known research studies and the state-of-the-art on affect recognition domain based on smartphone acquired data, namely smartphone embedded sensors and smartphone usage. Inevitably, supplementary modalities employed in many eminent studies are also reported here for the sake of completeness. Nevertheless, the intention of the survey is threefold; firstly to document all the to-date relevant literature on affect recognition through smartphone modalities, secondly to argue for the full potential of smartphone use in the inference of affect, and thirdly to demonstrate the current research trends towards mobile affective computing.},
author = {Politou, Eugenia and Alepis, Efthimios and Patsakis, Constantinos},
doi = {10.1016/j.cosrev.2017.07.002},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Politou, Alepis, Patsakis/Computer Science Review/Politou, Alepis, Patsakis - 2017 - A survey on mobile affective computing.pdf:pdf},
issn = {15740137},
journal = {Comput. Sci. Rev.},
keywords = {Affect detection,Affect recognition,Mobile affective computing,Mobile sensing,Smartphone sensors},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
publisher = {Elsevier Inc.},
title = {{A survey on mobile affective computing}},
url = {http://dx.doi.org/10.1016/j.cosrev.2017.07.002},
year = {2017}
}
@article{Gao2012,
abstract = {The increasing number of people playing games on touch-screen mobile phones raises the question of whether touch behaviors reflect players' emotional states. This prospect would not only be a valuable eval- uation indicator for game designers, but also for real-time personalization of the game experience. Psychol- ogy studies on acted touch behavior show the existence of discriminative affective profiles. In this article, finger-stroke features during gameplay on an iPod were extracted and their discriminative power analyzed. Machine learning algorithms were used to build systems for automatically discriminating between four emo- tional states (Excited, Relaxed, Frustrated, Bored), two levels of arousal and two levels of valence. Accuracy reached between 69{\%} and 77{\%} for the four emotional states, and higher results (∼89{\%}) were obtained for discriminating between two levels of arousal and two levels of valence. We conclude by discussing the factors relevant to the generalization of the results to applications other than games.},
author = {Gao, Yuan and Bianchi-Berthouze, Nadia and Meng, Hongying},
doi = {10.1145/2395131.2395138},
file = {:Users/changkun/Documents/Mendeley Desktop/2012/Gao, Bianchi-Berthouze, Meng/ACM Transactions on Computer-Human Interaction/Gao, Bianchi-Berthouze, Meng - 2012 - What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay.pdf:pdf},
isbn = {10730516 (ISSN)},
issn = {10730516},
journal = {ACM Trans. Comput. Interact.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {4},
pages = {1--30},
title = {{What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay?}},
url = {http://dl.acm.org/citation.cfm?doid=2395131.2395138},
volume = {19},
year = {2012}
}
@article{Shah2015,
abstract = {The role of affect and emotion in interactive system design is an active and recent research area. The aim is to make systems more responsive to user's needs and expectations. The first step towards affective interaction is to recognize user's emotional state. Literature contains many works on emotion recognition. In those works, facial muscle movement, gestures, postures and physiological signals were used for recognition. The methods are computation intensive and require extra hardware (e.g., sensors and wires). In this work, we propose a simpler model to predict the affective state of a touch screen user. The prediction is done based on the user's touch input, namely the finger strokes. We defined seven features based on the strokes. A linear combination of these features is proposed as the predictor, which can predict a user's affective state into one of the three states: positive (happy, excited and elated), negative (sad, anger, fear, disgust) and neutral (calm, relaxed and contented). The model alleviates the need for extra setup as well as extensive computation, making it suitable for implementation on mobile devices with limited resources. The model is developed and validated with empirical data involving 57 participants performing 7 touch input tasks. The validation study demonstrates a high prediction accuracy of 90.47 {\%}. The proposed model and its empirical development and validation are described in this paper.},
author = {Shah, Sachin and Teja, J. Narasimha and Bhattacharya, Samit},
doi = {10.1186/s40166-015-0013-z},
file = {:Users/changkun/Documents/Mendeley Desktop/2015/Shah, Teja, Bhattacharya/Journal of Interaction Science/Shah, Teja, Bhattacharya - 2015 - Towards affective touch interaction predicting mobile user emotion from finger strokes.pdf:pdf},
issn = {2194-0827},
journal = {J. Interact. Sci.},
keywords = {Emotional state,Touch screen,Strike and tap,Featur,emotional state,empirical study,features,linear regression,strike and tap,touch screen},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
pages = {6},
publisher = {Journal of Interaction Science},
title = {{Towards affective touch interaction: predicting mobile user emotion from finger strokes}},
url = {http://www.journalofinteractionscience.com/content/3/1/6},
volume = {3},
year = {2015}
}
@article{Dalvand,
author = {Dalvand, Kasra},
file = {:Users/changkun/Documents/Mendeley Desktop/Unknown/Dalvand/Unknown/Dalvand - Unknown - An Adaptive User-Interface Based on User ' s Emotion.pdf:pdf},
keywords = {-human},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{An Adaptive User-Interface Based on User ' s Emotion}}
}
@article{Galindo,
author = {Galindo, Juli{\'{a}}n Andr{\'{e}}s and Dupuy-chessa, Sophie and C{\'{e}}ret, {\'{E}}ric},
file = {:Users/changkun/Documents/Mendeley Desktop/Unknown/Galindo, Dupuy-chessa, C{\'{e}}ret/Unknown/Galindo, Dupuy-chessa, C{\'{e}}ret - Unknown - T OWARD A UI ADAPTATION APPROACH DRIVEN BY USER EMOTIONS.pdf:pdf},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
title = {{T OWARD A UI ADAPTATION APPROACH DRIVEN BY USER EMOTIONS}}
}
@article{Bhattacharya2017,
author = {Bhattacharya, Samit},
doi = {10.4018/IJMHCI.2017010103},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Bhattacharya/Unknown/Bhattacharya - 2017 - Model for Affective State Detection of Mobile Touch Screen Users.pdf:pdf},
isbn = {2017010103},
keywords = {emotional state,empirical study,features,linear regression,strike and tap,touch screen},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
pages = {10--13},
title = {{Model for Affective State Detection of Mobile Touch Screen Users}},
volume = {9},
year = {2017}
}
@article{Lentini2017,
author = {Lentini, Rodrigo C and Ionascu, Beatrice and Eyssel, Friederike A and Copti, Scandar and Eid, Mohamad},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Lentini et al/Unknown/Lentini et al. - 2017 - Authoring Tactile Gestures Case Study for Emotion Stimulation.pdf:pdf},
keywords = {arousal,reactions,tactile stimulation,valence},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Authoring Tactile Gestures : Case Study for Emotion Stimulation}},
year = {2017}
}
@INPROCEEDINGS{Hyun2012, 
author={Hyun-Jun Kim and Young Sang Choi}, 
booktitle={2012 IEEE Consumer Communications and Networking Conference (CCNC)}, 
title={Exploring emotional preference for smartphone applications}, 
year={2012}, 
volume={}, 
number={}, 
pages={245-249},
doi={10.1109/CCNC.2012.6181095}, 
ISSN={2331-9852}, 
month={Jan},
}
@book{darwin1998expression,
  title={The expression of the emotions in man and animals},
  author={Darwin, Charles and Prodger, Phillip},
  year={1998},
  publisher={Oxford University Press, USA}
}
@article{james1884emotion,
  title={What is an emotion?},
  author={James, William},
  journal={Mind},
  volume={9},
  number={34},
  pages={188--205},
  year={1884},
  publisher={JSTOR}
}
@book{james2013emotion,
  title={What is an Emotion?},
  author={James, William},
  year={2013},
  publisher={Simon and Schuster}
}
@book{turkle2005second,
  title={The second self: Computers and the human spirit},
  author={Turkle, Sherry},
  year={2005},
  publisher={Mit Press}
}
@misc{emotionmap,
  author = {stanchew},
  title = {{A map of human emotions}},
  howpublished = "\url{https://stanchew.wordpress.com/2012/04/23/a-map-of-human-emotions/}",
  year = {2012}, 
  note = "[Online; accessed 20-November-2017]"
}
@article{Picard1999,
abstract = {Not all computers need to pay attention to emotions, or to have emotional abilities. Some machines are useful as rigid tools, and it is fine to keep them that way. However, there are situations where the human-machine interaction could be improved by having machines naturally adapt to their users, and where communication about when, where, how, and how important it is to adapt involves emotional information, possibly including expressions of frustration, confusion, disliking, interest, and more. Affective computing expands humancomputer interaction by including emotional communication together with appropriate means of handling affective information. This paper highlights recent and ongoing work at the {\{}MIT{\}} Media Lab in affective computing, computing that relates to, arises from, or deliberately influences emotion. This work currently targets four broad areas related to {\{}HCI{\}}: (1) Reducing user frustration; (2) Enabling comfortable communication of user emotion; (3) Developing infrastructure and applications to handle affective information; and, (4) Building tools that help develop social-emotional skills.},
archivePrefix = {arXiv},
arxivId = {742338},
author = {Picard, Rosalind W},
eprint = {742338},
file = {:Users/changkun/Documents/Mendeley Desktop/Unknown/Picard/Unknown/Picard - Unknown - AFFECTIVE COMPUTING FOR HCI.pdf:pdf},
isbn = {0-8058-3391-9},
issn = {0001-5989},
journal = {Proceedings of the 8th HCI International on Human-Computer Interaction: Ergonomics and User Interfaces},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {829--833},
pmid = {9223372036854775808},
title = {{Affective Computing for HCI}},
url = {http://dl.acm.org/citation.cfm?id=647943.742338},
year = {1999}
}
@misc{Mollahosseini2017,
abstract = {Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.},
archivePrefix = {arXiv},
arxivId = {1708.03985},
author = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H.},
booktitle = {IEEE Transactions on Affective Computing},
doi = {10.1109/TAFFC.2017.2740923},
eprint = {1708.03985},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Mollahosseini, Hasani, Mahoor/IEEE Transactions on Affective Computing/Mollahosseini, Hasani, Mahoor - 2017 - AffectNet A Database for Facial Expression, Valence, and Arousal Computing in the Wild.pdf:pdf},
issn = {19493045},
keywords = {Affective computing in the wild,arousal,continuous dimensional space,facial expressions,valence},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {1--18},
title = {{AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild}},
year = {2017}
}
@article{bhattacharya2017predictive,
author = {Bhattacharya, S},
doi = {10.4018/IJMHCI.2017010103},
issn = {1942-390X},
journal = {International Journal of Mobile Human Computer Interaction},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
pages = {30--44},
publisher = {IGI Global},
title = {{A predictive linear regression model for affective state detection of mobile touch screen users}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994051845{\&}doi=10.4018{\%}2FIJMHCI.2017010103{\&}partnerID=40{\&}md5=a3c3eeecad6e2a24b3c416bd99e1d934},
volume = {9},
year = {2017}
}

@article{Tao2005,
abstract = {Affective computing is currently one of the most active research topics, furthermore, having increasingly intensive attention. This strong interest is driven by a wide spectrum of promising applications in many areas such as virtual reality, smart surveillance, perceptual interface, etc. Affective computing concerns multidisciplinary knowledge background such as psychology, cognitive, physiology and computer sciences. The paper is emphasized on the several issues involved implicitly in the whole interactive feedback loop. Various methods for each issue are discussed in order to examine the state of the art. Finally, some research challenges and future directions are also discussed.},
author = {Tao, Jianhua and Tan, Tieniu},
doi = {10.1007/11573548_125},
file = {:Users/changkun/Documents/Mendeley Desktop/2005/Tao, Tan/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Tao, Tan - 2005 - Affective computing A review.pdf:pdf},
isbn = {3540296212},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {981--995},
title = {{Affective computing: A review}},
volume = {3784 LNCS},
year = {2005}
}
@article{Bigham2014,
author = {Bigham, Jeffrey P and Bernstein, Michael S},
file = {:Users/changkun/Documents/Mendeley Desktop/2014/Bigham, Bernstein/The handbook of collective intelligence/Bigham, Bernstein - 2014 - Human-Computer Interaction and Collective Intelligence.pdf:pdf},
isbn = {9780262029810},
journal = {The handbook of collective intelligence},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Human-Computer Interaction and Collective Intelligence}},
year = {2014}
}
@article{Hassib2017,
abstract = {Textual communication via mobile phones suffers from a lack of context and emotional awareness. We present a mobile chat application, HeartChat, which integrates heart rate as a cue to increase awareness and empathy. Through a literature review and a focus group, we identified design dimensions important for heart rate augmented chats. We created three concepts showing heart rate per message, in real-time, or sending it ex-plicitly. We tested our system in a two week in-the-wild study with 14 participants (7 pairs). Interviews and questionnaires showed that HeartChat supports empathy between people, in particular close friends and partners. Sharing heart rate helped them to implicitly understand each other's context (e.g. lo-cation, physical activity) and emotional state, and sparked curiosity on special occasions. We discuss opportunities, chal-lenges, and design implications for enriching mobile chats with physiological sensing.},
author = {Hassib, Mariam and Buschek, Daniel and Wozniak, PawelW W and Alt, Florian},
doi = {10.1145/3025453.3025758},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Hassib et al/Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems/Hassib et al. - 2017 - HeartChat Heart Rate Augmented Mobile Chat to Support Empathy and Awareness.pdf:pdf},
isbn = {978-1-4503-4655-9},
journal = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
keywords = {affective computing,heart rate,instant messagingg,physiological sensing},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {2239--2251},
title = {{HeartChat: Heart Rate Augmented Mobile Chat to Support Empathy and Awareness}},
url = {http://doi.acm.org/10.1145/3025453.3025758},
year = {2017}
}
@article{Picard2003,
abstract = {A number of researchers around the world have built machines that recognize, express, model, communicate, and respond to emotional information, instances of ''affective computing.'' This article raises and responds to several criticisms of affective computing, articulating state-of-the art research challenges, especially with respect to affect in human-computer interaction. r},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Picard, Rosalind W.},
doi = {10.1016/S1071-5819(03)00052-1},
eprint = {arXiv:1011.1669v3},
file = {:Users/changkun/Documents/Mendeley Desktop/2003/Picard/International Journal of Human Computer Studies/Picard - 2003 - Affective computing Challenges.pdf:pdf},
isbn = {1071-5819},
issn = {10715819},
journal = {International Journal of Human Computer Studies},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1-2},
pages = {55--64},
pmid = {10545},
title = {{Affective computing: Challenges}},
volume = {59},
year = {2003}
}
@article{Fragopanagos2005,
abstract = {In this paper, we outline the approach we have developed to construct an emotion-recognising system. It is based on guidance from psychological studies of emotion, as well as from the nature of emotion in its interaction with attention. A neural network architecture is constructed to be able to handle the fusion of different modalities (facial features, prosody and lexical content in speech). Results from the network are given and their implications discussed, as are implications for future direction for the research. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Fragopanagos, N. and Taylor, J. G.},
doi = {10.1016/j.neunet.2005.03.006},
file = {:Users/changkun/Documents/Mendeley Desktop/2005/Fragopanagos, Taylor/Neural Networks/Fragopanagos, Taylor - 2005 - Emotion recognition in human-computer interaction.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Attention control,Emotion classification,Emotion data sets,Emotions,Face feature analysis,Feedback learning,Lexical content,Prosody,Relaxation,Sigma-pi neural networks},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {4},
pages = {389--405},
pmid = {15921887},
title = {{Emotion recognition in human-computer interaction}},
volume = {18},
year = {2005}
}
@article{Lopez-Martinez2017,
abstract = {Pain is a complex and subjective experience that poses a number of measurement challenges. While self-report by the patient is viewed as the gold standard of pain assessment, this approach fails when patients cannot verbally communicate pain intensity or lack normal mental abilities. Here, we present a pain intensity measurement method based on physiological signals. Specifically, we implement a multi-task learning approach based on neural networks that accounts for individual differences in pain responses while still leveraging data from across the population. We test our method in a dataset containing multi-modal physiological responses to nociceptive pain.},
archivePrefix = {arXiv},
arxivId = {1708.08755},
author = {Lopez-Martinez, Daniel and Picard, Rosalind},
eprint = {1708.08755},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Lopez-Martinez, Picard/Unknown/Lopez-Martinez, Picard - 2017 - Multi-task Neural Networks for Personalized Pain Recognition from Physiological Signals.pdf:pdf},
isbn = {9781538606803},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {3--6},
title = {{Multi-task Neural Networks for Personalized Pain Recognition from Physiological Signals}},
url = {http://arxiv.org/abs/1708.08755},
year = {2017}
}
@article{Hihn2016,
abstract = {Humans rely on body gestures and posture when communicating. This topic has been covered in great detail by researchers from various fields. Within this work, approaches to transfer findings in psychology and behavioral studies regarding the relation between gestures and emotions to machine learning methods, will be investigated. Knowledge about the users emotional state is important to achieve human like, natural HCI in modern technical systems. The main focus lies on discriminating between mental overload and mental underload, when completing a given task, which for instance can be useful in an e-tutorial system. Mental underload is a new term used to describe the state a person is in when completing a dull or boring task. A suggestion how the affective states of overload and underload can be expressed using the established notation in the Valence, Arousal and Dominance (V; A;D) space will be given. In a further step it will be shown how to select suitable features, such as gestures, movement and postural behavior patterns. Based on the features selected, a classifier is designed and trained capable of deciding whether a person is experiencing mental overload or underload.},
author = {Hihn, Heinke and Meudt, Sascha and Schwenker, Friedhelm},
doi = {10.1145/3009960.3009961},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Hihn, Meudt, Schwenker/Proceedings of the 2nd workshop on Emotion Representations and Modelling for Companion Systems - ERM4CT '16/Hihn, Meudt, Schwenker - 2016 - Inferring mental overload based on postural behavior and gestures.pdf:pdf},
isbn = {9781450345583},
journal = {Proceedings of the 2nd workshop on Emotion Representations and Modelling for Companion Systems - ERM4CT '16},
keywords = {affective computing,emotion recognition,ensemble meth-},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {1--4},
title = {{Inferring mental overload based on postural behavior and gestures}},
url = {http://dl.acm.org/citation.cfm?doid=3009960.3009961},
year = {2016}
}
@article{Melcer2016,
abstract = {In this paper, we present a study examining how individuals embody emotion within form. Our findings provide a general taxonomy of affective dimensions of shape consistent with and extending previous literature. We also show that ordinary people can reasonably construct embodied shapes using affective dimensions, and illustrate that emotion is conveyed through both visual dimensions and tactile manipulations of shape. Participants used three distinct strategies for embodiment of emotion through shape: the look of a shape (visual representation), creation of a shape symbolizing the experience of an intended emotion (metaphor), and by evoking the intended emotion in the creator through affective movements and manipulations during construction (motion). This work ties together and extends understanding around emotion and form in HCI subdomains such as tangible embodied interaction, emotional assessment, and user experience evaluation.},
author = {Melcer, Edward and Isbister, Katherine},
doi = {10.1145/2851581.2892361},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Melcer, Isbister/Late-Breaking Work Designing Interactive Systems/Melcer, Isbister - 2016 - Motion, Emotion, and Form Exploring Affective Dimensions of Shape.pdf:pdf},
isbn = {9781450340823},
journal = {Late-Breaking Work: Designing Interactive Systems},
keywords = {Affect,Author Keywords Form,Emotion,HCI),Miscellaneous},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {1430--1437},
title = {{Motion, Emotion, and Form: Exploring Affective Dimensions of Shape}},
url = {http://dx.doi.org/10.1145/2851581.2892361},
year = {2016}
}
@article{McDuff2016,
abstract = {We present a real-time facial expression recognition toolkit that can automatically code the expressions of multiple people simultaneously. The toolkit is available across major mobile and desktop platforms (Android, iOS, Windows). The system is trained on the world's largest dataset of facial expressions and has been optimized to operate on mobile devices and with very few false detections. The toolkit offers the potential for the design of novel interfaces that respond to users' emotional states based on their facial expressions. We present a demonstration application that provides real-time visualization of the expressions captured by the camera.},
author = {McDuff, Daniel and Mahmoud, Abdelrahman and Mavadati, Mohammad and Amr, May and Turcot, Jay and el Kaliouby, Rana},
doi = {10.1145/2851581.2890247},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/McDuff et al/Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA '16/McDuff et al. - 2016 - AFFDEX SDK A Cross-Platform Real-Time Multi-Face Expression Recognition Toolkit.pdf:pdf},
isbn = {9781450340823},
journal = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA '16},
keywords = {Author Keywords Facial expressions,Emotion},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {3723--3726},
title = {{AFFDEX SDK: A Cross-Platform Real-Time Multi-Face Expression Recognition Toolkit}},
url = {https://www.affectiva.com/wp-content/uploads/2017/03/McDuff{\_}2016{\_}Affdex.pdf{\%}0Ahttp://dl.acm.org/citation.cfm?doid=2851581.2890247},
year = {2016}
}
@article{Pike2015,
abstract = {{\textcopyright} 2015 ACM.A users interaction with a film typically involves a One Way Affect (1WA), in which the film being consumed has an affect on the consumer. Recent advances in physiological monitoring technology however has facilitated the notion of a Two Way Affect Loop (2WAL), in which a film piece can be dynamically affected by a consumers physiology or behaviour. This paper outlines an agenda for further investigating 2WAL, setting research questions and the inuence of related research areas.},
author = {Pike, Matthew and Ramchurn, Richard and Wilson, Max L},
doi = {10.1145/2783446.2783595},
file = {:Users/changkun/Documents/Mendeley Desktop/2015/Pike, Ramchurn, Wilson/Proceedings of the 2015 British HCI Conference/Pike, Ramchurn, Wilson - 2015 - Two-way Affect Loops in Multimedia Experiences.pdf:pdf},
isbn = {978-1-4503-3643-7},
journal = {Proceedings of the 2015 British HCI Conference},
keywords = {BCI,EEG,adaptive media,digital arts,physiology},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {117--118},
title = {{Two-way Affect Loops in Multimedia Experiences}},
url = {http://doi.acm.org/10.1145/2783446.2783595},
year = {2015}
}
@article{Pham2017,
abstract = {Understanding a target audience's emotional responses to video advertisements is crucial to stakeholders. However, traditional methods for collecting such information are slow, expensive, and coarse-grained. We propose AttentiveVideo, an intelligent mobile interface with corresponding inference algorithms to monitor and quantify the effects of mobile video advertising. AttentiveVideo employs a combination of implicit photoplethysmography (PPG) sensing and facial expression analysis (FEA) to predict viewers' attention, engagement, and sentiment when watching video advertisements on unmodified smartphones. In a 24-participant study, we found that AttentiveVideo achieved good accuracies on a wide range of emotional measures (the best average accuracy = 73.59{\%}, kappa = 0.46 across 9 metrics). We also found that the PPG sensing channel and the FEA technique are complimentary. While FEA works better for strong emotions (e.g., joy and anger), the PPG channel is more informative for subtle responses or emotions. These findings show the potential for both lowcost collection and deep understanding of emotional responses to mobile video advertisements. {\textcopyright} 2017 ACM.},
author = {Pham, Phuong and Wang, Jingtao},
doi = {10.1145/3025171.3025186},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Pham, Wang/Proceedings of the 22nd International Conference on Intelligent User Interfaces - IUI '17/Pham, Wang - 2017 - Understanding Emotional Responses to Mobile Video Advertisements via Physiological Signal Sensing and Facial Express.pdf:pdf},
isbn = {9781450343480},
journal = {Proceedings of the 22nd International Conference on Intelligent User Interfaces - IUI '17},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {67--78},
title = {{Understanding Emotional Responses to Mobile Video Advertisements via Physiological Signal Sensing and Facial Expression Analysis}},
url = {http://dl.acm.org/citation.cfm?doid=3025171.3025186},
year = {2017}
}
@article{Zeng2004,
abstract = {Perhaps the most fundamental application of affective computing would be Human-Computer Interaction (HCI) in which the computer is able to detect and track the user's affective states, and make corresponding feedback. The human multi-sensor affect system defines the expectation of multimodal affect analyzer. In this paper, we present our efforts toward audio-visual HCI-related affect recognition. With HCI applications in mind, we take into account some special affective states which indicate users' cognitive/motivational states. Facing the fact that a facial expression is influenced by both an affective state and speech content, we apply a smoothing method to extract the information of the affective state from facial features. In our fusion stage, a voting method is applied to combine audio and visual modalities so that the final affect recognition accuracy is greatly improved. We test our bimodal affect recognition approach on 38 subjects with 11 HCI-related affect states. The extensive experimental results show that the average person-dependent affect recognition accuracy is almost 90{\%} for our bimodal fusion. Copyright 2004 ACM.},
author = {Zeng, Zhihong and Tu, Jilin and Liu, Ming and Zhang, Tong and Rizzolo, Nicholas and Zhang, Zhenqiu and Huang, Thomas S. and Roth, Dan and Levinson, Stephen},
doi = {10.1145/1027933.1027958},
file = {:Users/changkun/Documents/Mendeley Desktop/2004/Zeng et al/Proceedings of the 6th international conference on Multimodal interfaces - ICMI '04/Zeng et al. - 2004 - Bimodal HCI-related affect recognition.pdf:pdf},
isbn = {1581139950},
journal = {Proceedings of the 6th international conference on Multimodal interfaces  - ICMI '04},
keywords = {affect recognition,affective,emotion recognition,hci,multimodal human-computer interaction},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {137},
title = {{Bimodal HCI-related affect recognition}},
url = {http://portal.acm.org/citation.cfm?doid=1027933.1027958},
year = {2004}
}
@article{Kim2016,
author = {Kim, Joohee and Lee, Na Hyeon and Bae, Byung-Chull and Cho, Jun Dong},
doi = {10.1145/2908805.2909414},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Kim et al/Proceedings of the 2016 ACM Conference Companion Publication on Designing Interactive Systems - DIS '16 Companion/Kim et al. - 2016 - A Feedback System for the Prevention of Forward Head Posture in Sedentary Work Environments.pdf:pdf},
isbn = {9781450343152},
journal = {Proceedings of the 2016 ACM Conference Companion Publication on Designing Interactive Systems - DIS '16 Companion},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {161--164},
title = {{A Feedback System for the Prevention of Forward Head Posture in Sedentary Work Environments}},
url = {http://dl.acm.org/citation.cfm?doid=2908805.2909414},
year = {2016}
}
@article{Cauchard2016,
abstract = {—Drones are becoming more popular and may soon be ubiquitous. As they enter our everyday environments, it becomes critical to ensure their usability through natural Human-Drone Interaction (HDI). Previous work in Human-Robot Interaction (HRI) shows that adding an emotional component is part of the key to success in robots' acceptability. We believe the adoption of personal drones would also benefit from adding an emotional component. This work defines a range of personality traits and emotional attributes that can be encoded in drones through their flight paths. We present a user study (N=20) and show how well three defined emotional states can be recognized. We draw conclusions on interaction techniques with drones and feedback strategies that use the drone's flight path and speed.},
author = {Cauchard, Jessica R. and Zhai, Kevin Y. and Spadafora, Marco and Landay, James A.},
doi = {10.1109/HRI.2016.7451761},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Cauchard et al/ACMIEEE International Conference on Human-Robot Interaction/Cauchard et al. - 2016 - Emotion encoding in human-drone interaction.pdf:pdf},
isbn = {9781467383707},
issn = {21672148},
journal = {ACM/IEEE International Conference on Human-Robot Interaction},
keywords = {Affective computing,Drone,UAV},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {263--270},
title = {{Emotion encoding in human-drone interaction}},
volume = {2016-April},
year = {2016}
}
@article{Tewell2017,
author = {Tewell, Jordan and Bird, Jon and Buchanan, George R.},
doi = {10.1145/3025453.3025844},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Tewell, Bird, Buchanan/Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI '17/Tewell, Bird, Buchanan - 2017 - The Heat is On A Temperature Display for Conveying Affective Feedback.pdf:pdf},
isbn = {9781450346559},
journal = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI '17},
keywords = {affective computing,thermal feedback,thermal haptics},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {1756--1767},
title = {{The Heat is On: A Temperature Display for Conveying Affective Feedback}},
url = {http://dl.acm.org/citation.cfm?doid=3025453.3025844},
year = {2017}
}
@article{Dermody2016,
abstract = {A multimodal system with real-time feedback for public speaking has been developed. The system has been developed within the paradigm of positive computing which focuses on designing for user wellbeing. To date we have focused on the following determinants of wellbeing – autonomy, self-awareness and stress reduction.},
author = {Dermody, Fiona},
doi = {10.1145/2993148.2997616},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Dermody/Proceedings of the 18th ACM International Conference on Multimodal Interaction - ICMI 2016/Dermody - 2016 - Multimodal positive computing system for public speaking with real-time feedback.pdf:pdf},
isbn = {9781450345569},
journal = {Proceedings of the 18th ACM International Conference on Multimodal Interaction - ICMI 2016},
keywords = {affective computing,hci,multimodal interfaces,positive computing,public speaking,real-time feedback},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {541--545},
title = {{Multimodal positive computing system for public speaking with real-time feedback}},
url = {http://dl.acm.org/citation.cfm?doid=2993148.2997616},
year = {2016}
}
@article{Naidu2015,
author = {Naidu, Ganapreeta R.},
doi = {10.1145/2818346.2823307},
file = {:Users/changkun/Documents/Mendeley Desktop/2015/Naidu/Proceedings of the 2015 ACM on International Conference on Multimodal Interaction - ICMI '15/Naidu - 2015 - A Computational Model of Culture-Specific Emotion Detection for Artificial Agents in the Learning Domain.pdf:pdf},
isbn = {9781450339124},
journal = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction - ICMI '15},
keywords = {affective computing,computer interaction,culture-specific,hci,human,intelligent agents,learning,multimodal interaction},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {635--639},
title = {{A Computational Model of Culture-Specific Emotion Detection for Artificial Agents in the Learning Domain}},
url = {http://dl.acm.org/citation.cfm?doid=2818346.2823307},
year = {2015}
}
@article{Crane2007,
abstract = {Emotion is a topic of growing interest in the HCI community. Studying emotion within the HCI discipline is an exciting interdisciplinary task. This can be facilitated by the exchange of thoughts and ideas with others working on related projects. The aim of this SIG is to bring together an interdisciplinary group of researchers and practitioners actively working on projects where emotion is an essential component. The goals of the SIG are to identify current themes related to emotion specific HCI work and discuss strategies for moving forward.},
author = {Crane, Elizabeth a and Shami, N Sadat and Peter, Christian},
doi = {10.1145/1240866.1240958},
file = {:Users/changkun/Documents/Mendeley Desktop/2007/Crane, Shami, Peter/Proceedings of ACM CHI 2007 Conference on Human Factors in Computing Systems/Crane, Shami, Peter - 2007 - Let's get emotional emotion research in human computer interaction.pdf:pdf},
isbn = {9781595936424},
journal = {Proceedings of ACM CHI 2007 Conference on Human Factors in Computing Systems},
keywords = {acm classification keywords,affective applications,affective computing,emotion,emotion detection,hci},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {2101--2104},
title = {{Let's get emotional: emotion research in human computer interaction}},
url = {http://doi.acm.org/10.1145/1240866.1240958},
volume = {2},
year = {2007}
}
@article{Eid2016,
author = {Eid, Mohamad A and Member, Senior and Osman, Hussein A L},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Eid, Member, Osman/Unknown/Eid, Member, Osman - 2016 - Affective Haptics Current Research and Future Directions.pdf:pdf},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Affective Haptics : Current Research and Future Directions}},
volume = {4},
year = {2016}
}
@article{Poria2017,
author = {Poria, Soujanya and Cambria, Erik and Bajpai, Rajiv and Hussain, Amir},
doi = {10.1016/j.inffus.2017.02.003},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Poria et al/Information Fusion/Poria et al. - 2017 - A review of affective computing From unimodal analysis to multimodal fusion.pdf:pdf},
issn = {1566-2535},
journal = {Information Fusion},
keywords = {"Audio,Affective computing,Multimodal affect analysis,Multimodal fusion,Sentiment analysis,ek laboratories,nanyang technological university,singapore,visual and text information fusion"},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {98--125},
publisher = {Elsevier B.V.},
title = {{A review of affective computing : From unimodal analysis to multimodal fusion}},
url = {http://dx.doi.org/10.1016/j.inffus.2017.02.003},
volume = {37},
year = {2017}
}
@article{Mazzoni2016,
author = {Mazzoni, Antonella and Bryan-kinns, Nick},
doi = {10.1016/j.entcom.2016.06.002},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Mazzoni, Bryan-kinns/Entertainment Computing/Mazzoni, Bryan-kinns - 2016 - Mood Glove A haptic wearable prototype system to enhance mood music in film.pdf:pdf},
issn = {1875-9521},
journal = {Entertainment Computing},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {9--17},
publisher = {The Authors},
title = {{Mood Glove : A haptic wearable prototype system to enhance mood music in film}},
url = {http://dx.doi.org/10.1016/j.entcom.2016.06.002},
volume = {17},
year = {2016}
}
@article{Lentini,
author = {Lentini, Rodrigo C and Ionascu, Beatrice and Eyssel, Friederike A and Copti, Scandar and Eid, Mohamad},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Lentini et al/Unknown/Lentini et al. - 2017 - Authoring Tactile Gestures Case Study for Emotion Stimulation.pdf:pdf},
keywords = {arousal,reactions,tactile stimulation,valence},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Authoring Tactile Gestures : Case Study for Emotion Stimulation}},
year = {2017}
}
@article{Wunarso2017,
author = {Wunarso, Novita Belinda and Soelistio, Yustinus Eko},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Wunarso, Soelistio/Unknown/Wunarso, Soelistio - 2017 - Towards Indonesian Speech-Emotion Automatic Recognition ( I-SpEAR ).pdf:pdf},
keywords = {3,a method by pan,al,and neutral was done,attempt to recognize three,by,daubechies wavelet,emotional states,et,happy,linear mixed effect,sad,speech-emotion recognition,svm},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {8--11},
title = {{Towards Indonesian Speech-Emotion Automatic Recognition ( I-SpEAR )}},
volume = {2017},
year = {2017}
}