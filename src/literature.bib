%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Bastian Pfleging at 2016-07-01 13:45:13 +0200 


%% Saved with string encoding Unicode (UTF-8) 

@book{Hutchison2011,
abstract = {Interest in emotion detection is increasing significantly. For research and development in the field of Affective Computing and emotion-aware interaction techniques, reliable and robust technology is needed for detecting emotional signs in users under everyday conditions. In this paper, a novel wearable system for measuring emotion-related physiological parameters is presented. Currently heart rate, skin conductivity, and skin temperature are taken; further sensors can easily be added. The system is very easy to use, robust, and suitable for mobile and long-time logging of data. It has an open architecture and can easily be integrated into other systems or applications. The system is designed for use in emotion research as well as in everyday affective applications.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Hutchison, David and Mitchell, John C},
doi = {10.1007/978-3-642-24600-5},
eprint = {9780201398298},
file = {:Users/changkun/Documents/Mendeley Desktop/2011/Hutchison, Mitchell/Unknown/Hutchison, Mitchell - 2011 - Affective Computing and Intelligent Interaction.pdf:pdf},
isbn = {978-3-642-24599-2},
issn = {03029743},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pmid = {4520227},
title = {{Affective Computing and Intelligent Interaction}},
url = {http://link.springer.com/10.1007/978-3-642-24600-5},
volume = {6974},
year = {2011}
}
@article{Bailenson2007,
abstract = {This article examines the phenomenon of Virtual Interpersonal Touch (VIT), people touching one another via force-feedback haptic devices. As collaborative virtual environments become utilized more effectively, it is only natural that interactants will have the ability to touch one another. In the work presented here, we used relatively basic devices to begin to explore the expression of emotion through VIT. In Experiment 1, participants utilizeda2DOF force-feedback joy- stick to express seven emotions.We examined various dimensions of the forces generated and subjective ratings of the difficulty of expressing those emotions. In Experiment 2, a separate group of participants attempted to recognize the recordings of emotions generated in Experiment 1. In Experiment 3, pairs of partici- pants attempted to communicate the seven emotions using physical handshakes. Results indicated that humans were above chance when recognizing emotions via VIT but not as accurate as people expressing emotions through nonmediated handshakes.We discuss a theoretical framework for understanding emotions ex- pressed through touch as well as the implications of the current findings for the utilization of VIT in human–computer interaction.},
author = {Bailenson, Jeremy N and Yee, Nick and Brave, Scott and Merget, Dan and Koslow, David},
doi = {10.1080/07370020701493509},
file = {:Users/changkun/Documents/Mendeley Desktop/2007/Bailenson et al/Human-Computer Interaction/Bailenson et al. - 2007 - Virtual interpersonal touch expressing and recognizing emotions through haptic devices.pdf:pdf},
isbn = {0737-0024},
issn = {0737-0024},
journal = {Human-Computer Interact.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {November},
pages = {325--353},
title = {{Virtual interpersonal touch: expressing and recognizing emotions through haptic devices}},
volume = {22},
year = {2007}
}
@article{Mottelson2016,
abstract = {Current techniques to computationally detect human affect often depend on specialized hardware, work only in laboratory settings, or require substantial individual training. We use sen- sors in commodity smartphones to estimate affect in the wild with no training time based on a link between affect and move- ment. The first experiment had 55 participants do touch inter- actions after exposure to positive or neutral emotion-eliciting films; negative affect resulted in faster but less precise inter- actions, in addition to differences in rotation and acceleration. Using off-the-shelf machine learning algorithms we report 89.1{\%} accuracy in binary affective classification, grouping participants by their self-assessments. A follow up experiment validated findings from the first experiment; the experiment collected naturally occurring affect of 127 participants, who again did touch interactions. Results demonstrate that affect has direct behavioral effect on mobile interaction and that affect detection using common smartphone sensors is feasible.},
author = {Mottelson, Aske and Hornb{\ae}k, Kasper},
doi = {10.1145/2971648.2971654},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Mottelson, Hornb{\ae}k/ACM International Joint Conference on Pervasive and Ubiquitous Computing/Mottelson, Hornb{\ae}k - 2016 - An affect detection technique using mobile commodity sensors in the wild.pdf:pdf},
isbn = {1450344615},
journal = {ACM Int. Jt. Conf. Pervasive Ubiquitous Comput.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {781--792},
title = {{An affect detection technique using mobile commodity sensors in the wild}},
year = {2016}
}
@article{Hertenstein2009,
abstract = {The study of emotional communication has focused predominantly on the facial and vocal channels but has ignored the tactile channel. Participants in the current study were allowed to touch an unacquainted partner on the whole body to communicate distinct emotions. Of interest was how accurately the person being touched decoded the intended emotions without seeing the tactile stimulation. The data indicated that anger, fear, disgust, love, gratitude, and sympathy were decoded at greater than chance levels, as well as happiness and sadness, 2 emotions that have not been shown to be communicated by touch to date. Moreover, fine-grained coding documented specific touch behaviors associated with different emotions. The findings are discussed in terms of their contribution to the study of emotion-related communication.},
author = {Hertenstein, Matthew J and Hertenstein, Matthew J and Holmes, Rachel and Holmes, Rachel and Mccullough, Margaret and Mccullough, Margaret and Keltner, Dacher and Keltner, Dacher},
doi = {10.1037/a0016108},
file = {:Users/changkun/Documents/Mendeley Desktop/2009/Hertenstein, Holmes, Mccullough/Emotion/Hertenstein, Holmes, Mccullough - 2009 - The Communication of Emotion via Touch.pdf:pdf},
issn = {1528-3542},
journal = {Emotion},
keywords = {communication,emotion,tactile,touch},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {4},
pages = {566 --573},
pmid = {19653781},
title = {{The Communication of Emotion via Touch}},
volume = {9},
year = {2009}
}
@article{bhattacharya2017predictive,
author = {Bhattacharya, S},
doi = {10.4018/IJMHCI.2017010103},
issn = {1942-390X},
journal = {Int. J. Mob. Hum. Comput. Interact.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
pages = {30--44},
publisher = {IGI Global},
title = {{A predictive linear regression model for affective state detection of mobile touch screen users}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994051845{\&}doi=10.4018{\%}2FIJMHCI.2017010103{\&}partnerID=40{\&}md5=a3c3eeecad6e2a24b3c416bd99e1d934},
volume = {9},
year = {2017}
}
@article{Rana,
author = {Rana, Rajib and Hume, Margee},
file = {:Users/changkun/Documents/Mendeley Desktop/Unknown/Rana, Hume/Unknown/Rana, Hume - Unknown - Opportunistic and Context - aware Affect Sensing on Smartphones The Concept , Challenges and Opportunities .pdf:pdf},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {1--13},
title = {{Opportunistic and Context - aware Affect Sensing on Smartphones : The Concept , Challenges and Opportunities .}}
}
@article{Politou2017,
abstract = {The spontaneous recognition of emotional states and personality traits of individuals has been puzzling researchers for years whereas pertinent studies demonstrating the progress in the field, despite their diversity, are still encouraging. This work surveys the most well-known research studies and the state-of-the-art on affect recognition domain based on smartphone acquired data, namely smartphone embedded sensors and smartphone usage. Inevitably, supplementary modalities employed in many eminent studies are also reported here for the sake of completeness. Nevertheless, the intention of the survey is threefold; firstly to document all the to-date relevant literature on affect recognition through smartphone modalities, secondly to argue for the full potential of smartphone use in the inference of affect, and thirdly to demonstrate the current research trends towards mobile affective computing.},
author = {Politou, Eugenia and Alepis, Efthimios and Patsakis, Constantinos},
doi = {10.1016/j.cosrev.2017.07.002},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Politou, Alepis, Patsakis/Computer Science Review/Politou, Alepis, Patsakis - 2017 - A survey on mobile affective computing.pdf:pdf},
issn = {15740137},
journal = {Comput. Sci. Rev.},
keywords = {Affect detection,Affect recognition,Mobile affective computing,Mobile sensing,Smartphone sensors},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
publisher = {Elsevier Inc.},
title = {{A survey on mobile affective computing}},
url = {http://dx.doi.org/10.1016/j.cosrev.2017.07.002},
year = {2017}
}
@article{Gao2012,
abstract = {The increasing number of people playing games on touch-screen mobile phones raises the question of whether touch behaviors reflect players' emotional states. This prospect would not only be a valuable eval- uation indicator for game designers, but also for real-time personalization of the game experience. Psychol- ogy studies on acted touch behavior show the existence of discriminative affective profiles. In this article, finger-stroke features during gameplay on an iPod were extracted and their discriminative power analyzed. Machine learning algorithms were used to build systems for automatically discriminating between four emo- tional states (Excited, Relaxed, Frustrated, Bored), two levels of arousal and two levels of valence. Accuracy reached between 69{\%} and 77{\%} for the four emotional states, and higher results (∼89{\%}) were obtained for discriminating between two levels of arousal and two levels of valence. We conclude by discussing the factors relevant to the generalization of the results to applications other than games.},
author = {Gao, Yuan and Bianchi-Berthouze, Nadia and Meng, Hongying},
doi = {10.1145/2395131.2395138},
file = {:Users/changkun/Documents/Mendeley Desktop/2012/Gao, Bianchi-Berthouze, Meng/ACM Transactions on Computer-Human Interaction/Gao, Bianchi-Berthouze, Meng - 2012 - What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay.pdf:pdf},
isbn = {10730516 (ISSN)},
issn = {10730516},
journal = {ACM Trans. Comput. Interact.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {4},
pages = {1--30},
title = {{What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay?}},
url = {http://dl.acm.org/citation.cfm?doid=2395131.2395138},
volume = {19},
year = {2012}
}
@article{Shah2015,
abstract = {The role of affect and emotion in interactive system design is an active and recent research area. The aim is to make systems more responsive to user's needs and expectations. The first step towards affective interaction is to recognize user's emotional state. Literature contains many works on emotion recognition. In those works, facial muscle movement, gestures, postures and physiological signals were used for recognition. The methods are computation intensive and require extra hardware (e.g., sensors and wires). In this work, we propose a simpler model to predict the affective state of a touch screen user. The prediction is done based on the user's touch input, namely the finger strokes. We defined seven features based on the strokes. A linear combination of these features is proposed as the predictor, which can predict a user's affective state into one of the three states: positive (happy, excited and elated), negative (sad, anger, fear, disgust) and neutral (calm, relaxed and contented). The model alleviates the need for extra setup as well as extensive computation, making it suitable for implementation on mobile devices with limited resources. The model is developed and validated with empirical data involving 57 participants performing 7 touch input tasks. The validation study demonstrates a high prediction accuracy of 90.47 {\%}. The proposed model and its empirical development and validation are described in this paper.},
author = {Shah, Sachin and Teja, J. Narasimha and Bhattacharya, Samit},
doi = {10.1186/s40166-015-0013-z},
file = {:Users/changkun/Documents/Mendeley Desktop/2015/Shah, Teja, Bhattacharya/Journal of Interaction Science/Shah, Teja, Bhattacharya - 2015 - Towards affective touch interaction predicting mobile user emotion from finger strokes.pdf:pdf},
issn = {2194-0827},
journal = {J. Interact. Sci.},
keywords = {Emotional state,Touch screen,Strike and tap,Featur,emotional state,empirical study,features,linear regression,strike and tap,touch screen},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
pages = {6},
publisher = {Journal of Interaction Science},
title = {{Towards affective touch interaction: predicting mobile user emotion from finger strokes}},
url = {http://www.journalofinteractionscience.com/content/3/1/6},
volume = {3},
year = {2015}
}
