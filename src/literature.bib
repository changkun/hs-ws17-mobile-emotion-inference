@article{Garcia-Garcia2017,
abstract = {Emotion detection has become one of the most important aspects to consider in any project related to Affective Computing. Due to the almost endless applications of this new discipline, the development of emotion detection technologies has brought up as a quite profitable opportunity in the corporate sector. Many start-up enterprises have emerged in the last years, dedicated almost exclusively to a specific type of emotion detection technology. In this paper, we present a thorough review of current technologies to detect human emotions. To this end, we explore the different sources from which emotions can be read, along with existing technologies developed to recognize them. We also explore some application domains in which this technology has been applied. This survey has let us identify the strengths and shortcomings of current technology for emotion detection. We conclude the survey highlighting the aspects that requires further research and development.},
author = {Garcia-Garcia, Jose Maria and Penichet, Victor M R and Lozano, Maria D},
doi = {10.1145/3123818.3123852},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Garcia-Garcia, Penichet, Lozano/Interacci{\'{o}}n 2017/Garcia-Garcia, Penichet, Lozano - 2017 - Emotion Detection A Technology review 1.pdf:pdf},
isbn = {9781450352291},
journal = {Interacci{\'{o}}n 2017},
keywords = {Affective Computing,emotion recognition,technologies},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Emotion Detection: A Technology review 1}},
year = {2017}
}
@article{Conati2005,
author = {Conati, Cristina and Marsella, Stacy and Paiva, Ana},
doi = {10.1145/1040830.1040838},
file = {:Users/changkun/Documents/Mendeley Desktop/2005/Conati, Marsella, Paiva/Proceedings of the 10th international conference on Intelligent user interfaces - IUI '05/Conati, Marsella, Paiva - 2005 - Affective interactions.pdf:pdf},
isbn = {1581138946},
journal = {Proc. 10th Int. Conf. Intell. user interfaces  - IUI '05},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {7},
title = {{Affective interactions}},
url = {http://portal.acm.org/citation.cfm?doid=1040830.1040838},
year = {2005}
}
@article{Kaiser2006,
author = {Kaiser, Robin and Oertel, Karina},
file = {:Users/changkun/Documents/Mendeley Desktop/2006/Kaiser, Oertel/Computer/Kaiser, Oertel - 2006 - Emotions in HCI – An Affective E-Learning System.pdf:pdf},
journal = {Computer (Long. Beach. Calif).},
keywords = {affective computing,are sensors for e,e-learning,evaluated and enhanced,g,hci,integrated in the glove,resistance and skin conductivity,sensor data are wirelessly,skin,transmitted and made available},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {105--106},
title = {{Emotions in HCI – An Affective E-Learning System}},
year = {2006}
}
@article{Zhang2014,
abstract = {This survey presents recent progress on Affective Computing (AC) using mobile devices. AC has been one of the most active research topics for decades. The primary limitation of traditional AC research refers to as impermeable emotions. This criticism is prominent when emotions are investigated outside social contexts. It is problematic because some emotions are directed at other people and arise from interactions with them. The development of smart mobile wearable devices (e.g., Apple Watch, Google Glass, iPhone, Fitbit) enables the wild and natural study for AC in the aspect of computer science. This survey emphasizes the AC study and system using smart wearable devices. Various models, methodologies and systems are discussed in order to examine the state of the art. Finally, we discuss remaining challenges and future works.},
archivePrefix = {arXiv},
arxivId = {1410.1648},
author = {Zhang, Shengkai and Hui, Pan},
eprint = {1410.1648},
file = {:Users/changkun/Documents/Mendeley Desktop/2014/Zhang, Hui/Unknown/Zhang, Hui - 2014 - A Survey on Mobile Affective Computing.pdf:pdf},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
title = {{A Survey on Mobile Affective Computing}},
url = {http://arxiv.org/abs/1410.1648},
year = {2014}
}
@book{Hutchison2011,
abstract = {Interest in emotion detection is increasing significantly. For research and development in the field of Affective Computing and emotion-aware interaction techniques, reliable and robust technology is needed for detecting emotional signs in users under everyday conditions. In this paper, a novel wearable system for measuring emotion-related physiological parameters is presented. Currently heart rate, skin conductivity, and skin temperature are taken; further sensors can easily be added. The system is very easy to use, robust, and suitable for mobile and long-time logging of data. It has an open architecture and can easily be integrated into other systems or applications. The system is designed for use in emotion research as well as in everyday affective applications.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Hutchison, David and Mitchell, John C},
doi = {10.1007/978-3-642-24600-5},
eprint = {9780201398298},
file = {:Users/changkun/Documents/Mendeley Desktop/2011/Hutchison, Mitchell/Unknown/Hutchison, Mitchell - 2011 - Affective Computing and Intelligent Interaction.pdf:pdf},
isbn = {978-3-642-24599-2},
issn = {03029743},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pmid = {4520227},
title = {{Affective Computing and Intelligent Interaction}},
url = {http://link.springer.com/10.1007/978-3-642-24600-5},
volume = {6974},
year = {2011}
}
@article{Bailenson2007,
abstract = {This article examines the phenomenon of Virtual Interpersonal Touch (VIT), people touching one another via force-feedback haptic devices. As collaborative virtual environments become utilized more effectively, it is only natural that interactants will have the ability to touch one another. In the work presented here, we used relatively basic devices to begin to explore the expression of emotion through VIT. In Experiment 1, participants utilizeda2DOF force-feedback joy- stick to express seven emotions.We examined various dimensions of the forces generated and subjective ratings of the difficulty of expressing those emotions. In Experiment 2, a separate group of participants attempted to recognize the recordings of emotions generated in Experiment 1. In Experiment 3, pairs of partici- pants attempted to communicate the seven emotions using physical handshakes. Results indicated that humans were above chance when recognizing emotions via VIT but not as accurate as people expressing emotions through nonmediated handshakes.We discuss a theoretical framework for understanding emotions ex- pressed through touch as well as the implications of the current findings for the utilization of VIT in human–computer interaction.},
author = {Bailenson, Jeremy N and Yee, Nick and Brave, Scott and Merget, Dan and Koslow, David},
doi = {10.1080/07370020701493509},
file = {:Users/changkun/Documents/Mendeley Desktop/2007/Bailenson et al/Human-Computer Interaction/Bailenson et al. - 2007 - Virtual interpersonal touch expressing and recognizing emotions through haptic devices.pdf:pdf},
isbn = {0737-0024},
issn = {0737-0024},
journal = {Human-Computer Interact.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {November},
pages = {325--353},
title = {{Virtual interpersonal touch: expressing and recognizing emotions through haptic devices}},
volume = {22},
year = {2007}
}
@article{Mottelson2016,
abstract = {Current techniques to computationally detect human affect often depend on specialized hardware, work only in laboratory settings, or require substantial individual training. We use sen- sors in commodity smartphones to estimate affect in the wild with no training time based on a link between affect and move- ment. The first experiment had 55 participants do touch inter- actions after exposure to positive or neutral emotion-eliciting films; negative affect resulted in faster but less precise inter- actions, in addition to differences in rotation and acceleration. Using off-the-shelf machine learning algorithms we report 89.1{\%} accuracy in binary affective classification, grouping participants by their self-assessments. A follow up experiment validated findings from the first experiment; the experiment collected naturally occurring affect of 127 participants, who again did touch interactions. Results demonstrate that affect has direct behavioral effect on mobile interaction and that affect detection using common smartphone sensors is feasible.},
author = {Mottelson, Aske and Hornb{\ae}k, Kasper},
doi = {10.1145/2971648.2971654},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Mottelson, Hornb{\ae}k/ACM International Joint Conference on Pervasive and Ubiquitous Computing/Mottelson, Hornb{\ae}k - 2016 - An affect detection technique using mobile commodity sensors in the wild.pdf:pdf},
isbn = {1450344615},
journal = {ACM Int. Jt. Conf. Pervasive Ubiquitous Comput.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {781--792},
title = {{An affect detection technique using mobile commodity sensors in the wild}},
year = {2016}
}
@article{Hertenstein2009,
abstract = {The study of emotional communication has focused predominantly on the facial and vocal channels but has ignored the tactile channel. Participants in the current study were allowed to touch an unacquainted partner on the whole body to communicate distinct emotions. Of interest was how accurately the person being touched decoded the intended emotions without seeing the tactile stimulation. The data indicated that anger, fear, disgust, love, gratitude, and sympathy were decoded at greater than chance levels, as well as happiness and sadness, 2 emotions that have not been shown to be communicated by touch to date. Moreover, fine-grained coding documented specific touch behaviors associated with different emotions. The findings are discussed in terms of their contribution to the study of emotion-related communication.},
author = {Hertenstein, Matthew J and Hertenstein, Matthew J and Holmes, Rachel and Holmes, Rachel and Mccullough, Margaret and Mccullough, Margaret and Keltner, Dacher and Keltner, Dacher},
doi = {10.1037/a0016108},
file = {:Users/changkun/Documents/Mendeley Desktop/2009/Hertenstein et al/Emotion/Hertenstein et al. - 2009 - The Communication of Emotion via Touch.pdf:pdf},
issn = {1528-3542},
journal = {Emotion},
keywords = {communication,emotion,tactile,touch},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {4},
pages = {566 --573},
pmid = {19653781},
title = {{The Communication of Emotion via Touch}},
volume = {9},
year = {2009}
}
@article{bhattacharya2017predictive,
author = {Bhattacharya, S},
doi = {10.4018/IJMHCI.2017010103},
issn = {1942-390X},
journal = {Int. J. Mob. Hum. Comput. Interact.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
pages = {30--44},
publisher = {IGI Global},
title = {{A predictive linear regression model for affective state detection of mobile touch screen users}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994051845{\&}doi=10.4018{\%}2FIJMHCI.2017010103{\&}partnerID=40{\&}md5=a3c3eeecad6e2a24b3c416bd99e1d934},
volume = {9},
year = {2017}
}
@article{Rana,
author = {Rana, Rajib and Hume, Margee},
file = {:Users/changkun/Documents/Mendeley Desktop/Unknown/Rana, Hume/Unknown/Rana, Hume - Unknown - Opportunistic and Context - aware Affect Sensing on Smartphones The Concept , Challenges and Opportunities .pdf:pdf},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {1--13},
title = {{Opportunistic and Context - aware Affect Sensing on Smartphones : The Concept , Challenges and Opportunities .}}
}
@article{Politou2017,
abstract = {The spontaneous recognition of emotional states and personality traits of individuals has been puzzling researchers for years whereas pertinent studies demonstrating the progress in the field, despite their diversity, are still encouraging. This work surveys the most well-known research studies and the state-of-the-art on affect recognition domain based on smartphone acquired data, namely smartphone embedded sensors and smartphone usage. Inevitably, supplementary modalities employed in many eminent studies are also reported here for the sake of completeness. Nevertheless, the intention of the survey is threefold; firstly to document all the to-date relevant literature on affect recognition through smartphone modalities, secondly to argue for the full potential of smartphone use in the inference of affect, and thirdly to demonstrate the current research trends towards mobile affective computing.},
author = {Politou, Eugenia and Alepis, Efthimios and Patsakis, Constantinos},
doi = {10.1016/j.cosrev.2017.07.002},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Politou, Alepis, Patsakis/Computer Science Review/Politou, Alepis, Patsakis - 2017 - A survey on mobile affective computing.pdf:pdf},
issn = {15740137},
journal = {Comput. Sci. Rev.},
keywords = {Affect detection,Affect recognition,Mobile affective computing,Mobile sensing,Smartphone sensors},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
publisher = {Elsevier Inc.},
title = {{A survey on mobile affective computing}},
url = {http://dx.doi.org/10.1016/j.cosrev.2017.07.002},
year = {2017}
}
@article{Gao2012,
abstract = {The increasing number of people playing games on touch-screen mobile phones raises the question of whether touch behaviors reflect players' emotional states. This prospect would not only be a valuable eval- uation indicator for game designers, but also for real-time personalization of the game experience. Psychol- ogy studies on acted touch behavior show the existence of discriminative affective profiles. In this article, finger-stroke features during gameplay on an iPod were extracted and their discriminative power analyzed. Machine learning algorithms were used to build systems for automatically discriminating between four emo- tional states (Excited, Relaxed, Frustrated, Bored), two levels of arousal and two levels of valence. Accuracy reached between 69{\%} and 77{\%} for the four emotional states, and higher results (∼89{\%}) were obtained for discriminating between two levels of arousal and two levels of valence. We conclude by discussing the factors relevant to the generalization of the results to applications other than games.},
author = {Gao, Yuan and Bianchi-Berthouze, Nadia and Meng, Hongying},
doi = {10.1145/2395131.2395138},
file = {:Users/changkun/Documents/Mendeley Desktop/2012/Gao, Bianchi-Berthouze, Meng/ACM Transactions on Computer-Human Interaction/Gao, Bianchi-Berthouze, Meng - 2012 - What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay.pdf:pdf},
isbn = {10730516 (ISSN)},
issn = {10730516},
journal = {ACM Trans. Comput. Interact.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {4},
pages = {1--30},
title = {{What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay?}},
url = {http://dl.acm.org/citation.cfm?doid=2395131.2395138},
volume = {19},
year = {2012}
}
@article{Shah2015,
abstract = {The role of affect and emotion in interactive system design is an active and recent research area. The aim is to make systems more responsive to user's needs and expectations. The first step towards affective interaction is to recognize user's emotional state. Literature contains many works on emotion recognition. In those works, facial muscle movement, gestures, postures and physiological signals were used for recognition. The methods are computation intensive and require extra hardware (e.g., sensors and wires). In this work, we propose a simpler model to predict the affective state of a touch screen user. The prediction is done based on the user's touch input, namely the finger strokes. We defined seven features based on the strokes. A linear combination of these features is proposed as the predictor, which can predict a user's affective state into one of the three states: positive (happy, excited and elated), negative (sad, anger, fear, disgust) and neutral (calm, relaxed and contented). The model alleviates the need for extra setup as well as extensive computation, making it suitable for implementation on mobile devices with limited resources. The model is developed and validated with empirical data involving 57 participants performing 7 touch input tasks. The validation study demonstrates a high prediction accuracy of 90.47 {\%}. The proposed model and its empirical development and validation are described in this paper.},
author = {Shah, Sachin and Teja, J. Narasimha and Bhattacharya, Samit},
doi = {10.1186/s40166-015-0013-z},
file = {:Users/changkun/Documents/Mendeley Desktop/2015/Shah, Teja, Bhattacharya/Journal of Interaction Science/Shah, Teja, Bhattacharya - 2015 - Towards affective touch interaction predicting mobile user emotion from finger strokes.pdf:pdf},
issn = {2194-0827},
journal = {J. Interact. Sci.},
keywords = {Emotional state,Touch screen,Strike and tap,Featur,emotional state,empirical study,features,linear regression,strike and tap,touch screen},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
pages = {6},
publisher = {Journal of Interaction Science},
title = {{Towards affective touch interaction: predicting mobile user emotion from finger strokes}},
url = {http://www.journalofinteractionscience.com/content/3/1/6},
volume = {3},
year = {2015}
}
@article{Dalvand,
author = {Dalvand, Kasra},
file = {:Users/changkun/Documents/Mendeley Desktop/Unknown/Dalvand/Unknown/Dalvand - Unknown - An Adaptive User-Interface Based on User ' s Emotion.pdf:pdf},
keywords = {-human},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{An Adaptive User-Interface Based on User ' s Emotion}}
}
@article{Galindo,
author = {Galindo, Juli{\'{a}}n Andr{\'{e}}s and Dupuy-chessa, Sophie and C{\'{e}}ret, {\'{E}}ric},
file = {:Users/changkun/Documents/Mendeley Desktop/Unknown/Galindo, Dupuy-chessa, C{\'{e}}ret/Unknown/Galindo, Dupuy-chessa, C{\'{e}}ret - Unknown - T OWARD A UI ADAPTATION APPROACH DRIVEN BY USER EMOTIONS.pdf:pdf},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
title = {{T OWARD A UI ADAPTATION APPROACH DRIVEN BY USER EMOTIONS}}
}
@article{Bhattacharya2017,
author = {Bhattacharya, Samit},
doi = {10.4018/IJMHCI.2017010103},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Bhattacharya/Unknown/Bhattacharya - 2017 - Model for Affective State Detection of Mobile Touch Screen Users.pdf:pdf},
isbn = {2017010103},
keywords = {emotional state,empirical study,features,linear regression,strike and tap,touch screen},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
pages = {10--13},
title = {{Model for Affective State Detection of Mobile Touch Screen Users}},
volume = {9},
year = {2017}
}
@article{Lentini2017,
author = {Lentini, Rodrigo C and Ionascu, Beatrice and Eyssel, Friederike A and Copti, Scandar and Eid, Mohamad},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Lentini et al/Unknown/Lentini et al. - 2017 - Authoring Tactile Gestures Case Study for Emotion Stimulation.pdf:pdf},
keywords = {arousal,reactions,tactile stimulation,valence},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Authoring Tactile Gestures : Case Study for Emotion Stimulation}},
year = {2017}
}
@article{Mazzoni2016,
author = {Mazzoni, Antonella and Bryan-kinns, Nick},
doi = {10.1016/j.entcom.2016.06.002},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Mazzoni, Bryan-kinns/Entertainment Computing/Mazzoni, Bryan-kinns - 2016 - Mood Glove A haptic wearable prototype system to enhance mood music in film.pdf:pdf},
issn = {1875-9521},
journal = {Entertain. Comput.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {9--17},
publisher = {The Authors},
title = {{Mood Glove : A haptic wearable prototype system to enhance mood music in film}},
url = {http://dx.doi.org/10.1016/j.entcom.2016.06.002},
volume = {17},
year = {2016}
}

@article{Mazzoni2016,
author = {Mazzoni, Antonella and Bryan-kinns, Nick},
doi = {10.1016/j.entcom.2016.06.002},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Mazzoni, Bryan-kinns/Entertainment Computing/Mazzoni, Bryan-kinns - 2016 - Mood Glove A haptic wearable prototype system to enhance mood music in film.pdf:pdf},
issn = {1875-9521},
journal = {Entertain. Comput.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {9--17},
publisher = {The Authors},
title = {{Mood Glove : A haptic wearable prototype system to enhance mood music in film}},
url = {http://dx.doi.org/10.1016/j.entcom.2016.06.002},
volume = {17},
year = {2016}
}
@INPROCEEDINGS{Hyun2012, 
author={Hyun-Jun Kim and Young Sang Choi}, 
booktitle={2012 IEEE Consumer Communications and Networking Conference (CCNC)}, 
title={Exploring emotional preference for smartphone applications}, 
year={2012}, 
volume={}, 
number={}, 
pages={245-249}, 
keywords={accelerometers;behavioural sciences computing;emotion recognition;gyroscopes;haptic interfaces;matrix algebra;smart phones;touch sensitive screens;accelerometer;embedded sensor;emotional preference;gyroscope;human behaviour;human emotion recognition;off-line analysis;preference matrix;smartphone application;touch interface;user touch behaviour;Decision trees;Emotion recognition;Humans;Mobile communication;Psychology;Sensors;Speech;Affective Computing;Emotion Recognition;Machine Intelligence;Mobile Computing}, 
abstract={Emotion is an essential element of human behaviours. In this research, we investigated human behaviours related with the touch interface on a smartphone as a way to understand users' emotional states. As modern smartphones have various embedded sensors such as accelerometer and gyroscope, we aim to utilize data from these embedded sensors for recognizing human emotion and further finding emotional preferences for smartphone applications. We collected 12 attributes from 3 sensors during users' touch behaviours, and recognized seven basic emotions with off-line analysis. Finally, we generated a preference matrix of applications by calculating the difference between prior and posterior emotional states to the application usage. The pilot study showed 0.57 of an average f1-measure score (and 0.82 with decision tree based methods) with 455 cases of touch behaviour. We discovered that a simple touching behaviour has a potential for recognizing users' emotional states.}
doi={10.1109/CCNC.2012.6181095}, 
ISSN={2331-9852}, 
month={Jan},
}