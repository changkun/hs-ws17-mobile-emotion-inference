% ====================================================================================
% start introduction
% ====================================================================================
@article{james1884emotion,
  title={What is an emotion?},
  author={James, William},
  journal={Mind},
  volume={9},
  number={34},
  pages={188--205},
  year={1884},
  publisher={JSTOR}
}
@book{turkle2005second,
  title={The second self: Computers and the human spirit},
  author={Turkle, Sherry},
  year={2005},
  publisher={Mit Press}
}
@inproceedings{picard1999affective,
  title={Affective Computing for HCI.},
  author={Picard, Rosalind W},
  booktitle={HCI (1)},
  pages={829--833},
  year={1999}
}
@article{weiser1991computer,
  title={The computer for the 21st century},
  author={Weiser, Mark},
  journal={Scientific american},
  volume={265},
  number={3},
  pages={94--104},
  year={1991},
  publisher={New York}
}
@article{starner1996human,
  title={Human-powered wearable computing},
  author={Starner, Thad},
  journal={IBM systems Journal},
  volume={35},
  number={3.4},
  pages={618--629},
  year={1996},
  publisher={IBM}
}
% ====================================================================================
% end introduction
% ====================================================================================


































% ====================================================================================
% start data sources
% ====================================================================================
@article{Tao2005,
doi = {10.1007/11573548_125},
isbn = {3540296212},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {981--995},
title = {{Affective computing: A review}},
volume = {3784 LNCS},
year = {2005}
}
@article{Mazzoni2016,
author = {Mazzoni, Antonella and Bryan-kinns, Nick},
doi = {10.1016/j.entcom.2016.06.002},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Mazzoni, Bryan-kinns/Entertainment Computing/Mazzoni, Bryan-kinns - 2016 - Mood Glove A haptic wearable prototype system to enhance mood music in film.pdf:pdf},
issn = {1875-9521},
journal = {Entertainment Computing},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {9--17},
publisher = {The Authors},
title = {{Mood Glove : A haptic wearable prototype system to enhance mood music in film}},
url = {http://dx.doi.org/10.1016/j.entcom.2016.06.002},
volume = {17},
year = {2016}
}
@article{Poria2017,
author = {Poria, Soujanya and Cambria, Erik and Bajpai, Rajiv and Hussain, Amir},
doi = {10.1016/j.inffus.2017.02.003},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Poria et al/Information Fusion/Poria et al. - 2017 - A review of affective computing From unimodal analysis to multimodal fusion.pdf:pdf},
issn = {1566-2535},
journal = {Information Fusion},
keywords = {"Audio,Affective computing,Multimodal affect analysis,Multimodal fusion,Sentiment analysis,ek laboratories,nanyang technological university,singapore,visual and text information fusion"},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {98--125},
publisher = {Elsevier B.V.},
title = {{A review of affective computing : From unimodal analysis to multimodal fusion}},
url = {http://dx.doi.org/10.1016/j.inffus.2017.02.003},
volume = {37},
year = {2017}
}
@article{tan2013connectivity,
  title={Connectivity-based and anchor-free localization in large-scale 2d/3d sensor networks},
  author={Tan, Guang and Jiang, Hongbo and Zhang, Shengkai and Yin, Zhimeng and Kermarrec, Anne-Marie},
  journal={ACM Transactions on Sensor Networks (TOSN)},
  volume={10},
  number={1},
  pages={6},
  year={2013},
  publisher={ACM}
}
@article{Eid2016,
author = {Eid, Mohamad A and Member, Senior and Osman, Hussein A L},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Eid, Member, Osman/Unknown/Eid, Member, Osman - 2016 - Affective Haptics Current Research and Future Directions.pdf:pdf},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Affective Haptics : Current Research and Future Directions}},
volume = {4},
year = {2016}
}
@article{Lentini2017,
author = {Lentini, Rodrigo C and Ionascu, Beatrice and Eyssel, Friederike A and Copti, Scandar and Eid, Mohamad},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Lentini et al/Unknown/Lentini et al. - 2017 - Authoring Tactile Gestures Case Study for Emotion Stimulation.pdf:pdf},
keywords = {arousal,reactions,tactile stimulation,valence},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Authoring Tactile Gestures : Case Study for Emotion Stimulation}},
year = {2017}
}

% ====================================================================================
% end data sources
% ====================================================================================




























@article{Rajalakshmi2017ACS,
  title={A comprehensive survey on sentiment analysis},
  author={Srinivasan Rajalakshmi and Sarah John Asha and N. Pazhaniraja},
  journal={2017 Fourth International Conference on Signal Processing, Communication and Networking (ICSCN)},
  year={2017},
  pages={1-5}
}
@inproceedings{google2017,
title = {Attention is All You Need},
author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
year  = {2017},
URL = {https://arxiv.org/pdf/1706.03762.pdf}
}


@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  volume={2},
  pages={3},
  year={2010}
}

@article{kamper2017segmental,
  title={A segmental framework for fully-unsupervised large-vocabulary speech recognition},
  author={Kamper, Herman and Jansen, Aren and Goldwater, Sharon},
  journal={Computer Speech \& Language},
  volume={46},
  pages={154--174},
  year={2017},
  publisher={Elsevier}
}
@article{poria2016fusing,
  title={Fusing audio, visual and textual clues for sentiment analysis from multimodal content},
  author={Poria, Soujanya and Cambria, Erik and Howard, Newton and Huang, Guang-Bin and Hussain, Amir},
  journal={Neurocomputing},
  volume={174},
  pages={50--59},
  year={2016},
  publisher={Elsevier}
}

@article{d2015review,
  title={A review and meta-analysis of multimodal affect detection systems},
  author={D'mello, Sidney K and Kory, Jacqueline},
  journal={ACM Computing Surveys (CSUR)},
  volume={47},
  number={3},
  pages={43},
  year={2015},
  publisher={ACM}
}

@article{Garcia-Garcia2017,
author = {Garcia-Garcia, Jose Maria and Penichet, Victor M R and Lozano, Maria D},
doi = {10.1145/3123818.3123852},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Garcia-Garcia, Penichet, Lozano/Interacci{\'{o}}n 2017/Garcia-Garcia, Penichet, Lozano - 2017 - Emotion Detection A Technology review 1.pdf:pdf},
isbn = {9781450352291},
journal = {Interacci{\'{o}}n 2017},
keywords = {Affective Computing,emotion recognition,technologies},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Emotion Detection: A Technology review 1}},
year = {2017}
}
@article{Bailenson2007,
author = {Bailenson, Jeremy N and Yee, Nick and Brave, Scott and Merget, Dan and Koslow, David},
doi = {10.1080/07370020701493509},
file = {:Users/changkun/Documents/Mendeley Desktop/2007/Bailenson et al/Human-Computer Interaction/Bailenson et al. - 2007 - Virtual interpersonal touch expressing and recognizing emotions through haptic devices.pdf:pdf},
isbn = {0737-0024},
issn = {0737-0024},
journal = {Human-Computer Interact.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {November},
pages = {325--353},
title = {{Virtual interpersonal touch: expressing and recognizing emotions through haptic devices}},
volume = {22},
year = {2007}
}
@article{Mottelson2016,
author = {Mottelson, Aske and Hornb{\ae}k, Kasper},
doi = {10.1145/2971648.2971654},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Mottelson, Hornb{\ae}k/ACM International Joint Conference on Pervasive and Ubiquitous Computing/Mottelson, Hornb{\ae}k - 2016 - An affect detection technique using mobile commodity sensors in the wild.pdf:pdf},
isbn = {1450344615},
journal = {ACM Int. Jt. Conf. Pervasive Ubiquitous Comput.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {781--792},
title = {{An affect detection technique using mobile commodity sensors in the wild}},
year = {2016}
}
@article{hertenstein2009communication,
  title={The communication of emotion via touch.},
  author={Hertenstein, Matthew J and Holmes, Rachel and McCullough, Margaret and Keltner, Dacher},
  journal={Emotion},
  volume={9},
  number={4},
  pages={566},
  year={2009},
  publisher={American Psychological Association}
}

@article{Gao2012,
abstract = {The increasing number of people playing games on touch-screen mobile phones raises the question of whether touch behaviors reflect players' emotional states. This prospect would not only be a valuable eval- uation indicator for game designers, but also for real-time personalization of the game experience. Psychol- ogy studies on acted touch behavior show the existence of discriminative affective profiles. In this article, finger-stroke features during gameplay on an iPod were extracted and their discriminative power analyzed. Machine learning algorithms were used to build systems for automatically discriminating between four emo- tional states (Excited, Relaxed, Frustrated, Bored), two levels of arousal and two levels of valence. Accuracy reached between 69{\%} and 77{\%} for the four emotional states, and higher results (∼89{\%}) were obtained for discriminating between two levels of arousal and two levels of valence. We conclude by discussing the factors relevant to the generalization of the results to applications other than games.},
author = {Gao, Yuan and Bianchi-Berthouze, Nadia and Meng, Hongying},
doi = {10.1145/2395131.2395138},
file = {:Users/changkun/Documents/Mendeley Desktop/2012/Gao, Bianchi-Berthouze, Meng/ACM Transactions on Computer-Human Interaction/Gao, Bianchi-Berthouze, Meng - 2012 - What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay.pdf:pdf},
isbn = {10730516 (ISSN)},
issn = {10730516},
journal = {ACM Trans. Comput. Interact.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {4},
pages = {1--30},
title = {{What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay?}},
url = {http://dl.acm.org/citation.cfm?doid=2395131.2395138},
volume = {19},
year = {2012}
}
@article{Shah2015,
abstract = {The role of affect and emotion in interactive system design is an active and recent research area. The aim is to make systems more responsive to user's needs and expectations. The first step towards affective interaction is to recognize user's emotional state. Literature contains many works on emotion recognition. In those works, facial muscle movement, gestures, postures and physiological signals were used for recognition. The methods are computation intensive and require extra hardware (e.g., sensors and wires). In this work, we propose a simpler model to predict the affective state of a touch screen user. The prediction is done based on the user's touch input, namely the finger strokes. We defined seven features based on the strokes. A linear combination of these features is proposed as the predictor, which can predict a user's affective state into one of the three states: positive (happy, excited and elated), negative (sad, anger, fear, disgust) and neutral (calm, relaxed and contented). The model alleviates the need for extra setup as well as extensive computation, making it suitable for implementation on mobile devices with limited resources. The model is developed and validated with empirical data involving 57 participants performing 7 touch input tasks. The validation study demonstrates a high prediction accuracy of 90.47 {\%}. The proposed model and its empirical development and validation are described in this paper.},
author = {Shah, Sachin and Teja, J. Narasimha and Bhattacharya, Samit},
doi = {10.1186/s40166-015-0013-z},
file = {:Users/changkun/Documents/Mendeley Desktop/2015/Shah, Teja, Bhattacharya/Journal of Interaction Science/Shah, Teja, Bhattacharya - 2015 - Towards affective touch interaction predicting mobile user emotion from finger strokes.pdf:pdf},
issn = {2194-0827},
journal = {J. Interact. Sci.},
keywords = {Emotional state,Touch screen,Strike and tap,Featur,emotional state,empirical study,features,linear regression,strike and tap,touch screen},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
pages = {6},
publisher = {Journal of Interaction Science},
title = {{Towards affective touch interaction: predicting mobile user emotion from finger strokes}},
url = {http://www.journalofinteractionscience.com/content/3/1/6},
volume = {3},
year = {2015}
}

@article{Bhattacharya2017,
author = {Bhattacharya, Samit},
doi = {10.4018/IJMHCI.2017010103},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Bhattacharya/Unknown/Bhattacharya - 2017 - Model for Affective State Detection of Mobile Touch Screen Users.pdf:pdf},
isbn = {2017010103},
keywords = {emotional state,empirical study,features,linear regression,strike and tap,touch screen},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
pages = {10--13},
title = {{Model for Affective State Detection of Mobile Touch Screen Users}},
volume = {9},
year = {2017}
}

@INPROCEEDINGS{Hyun2012, 
author={Hyun-Jun Kim and Young Sang Choi}, 
booktitle={2012 IEEE Consumer Communications and Networking Conference (CCNC)}, 
title={Exploring emotional preference for smartphone applications}, 
year={2012}, 
volume={}, 
number={}, 
pages={245-249},
doi={10.1109/CCNC.2012.6181095}, 
ISSN={2331-9852}, 
month={Jan},
}
@book{darwin1998expression,
  title={The expression of the emotions in man and animals},
  author={Darwin, Charles and Prodger, Phillip},
  year={1998},
  publisher={Oxford University Press, USA}
}

@book{james2013emotion,
  title={What is an Emotion?},
  author={James, William},
  year={2013},
  publisher={Simon and Schuster}
}

@misc{emotionmap,
  author = {stanchew},
  title = {{A map of human emotions}},
  howpublished = "\url{https://stanchew.wordpress.com/2012/04/23/a-map-of-human-emotions/}",
  year = {2012}, 
  note = "[Online; accessed 20-November-2017]"
}
@misc{Mollahosseini2017,
abstract = {Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.},
archivePrefix = {arXiv},
arxivId = {1708.03985},
author = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H.},
booktitle = {IEEE Transactions on Affective Computing},
doi = {10.1109/TAFFC.2017.2740923},
eprint = {1708.03985},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Mollahosseini, Hasani, Mahoor/IEEE Transactions on Affective Computing/Mollahosseini, Hasani, Mahoor - 2017 - AffectNet A Database for Facial Expression, Valence, and Arousal Computing in the Wild.pdf:pdf},
issn = {19493045},
keywords = {Affective computing in the wild,arousal,continuous dimensional space,facial expressions,valence},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {1--18},
title = {{AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild}},
year = {2017}
}
@article{bhattacharya2017predictive,
author = {Bhattacharya, S},
doi = {10.4018/IJMHCI.2017010103},
issn = {1942-390X},
journal = {International Journal of Mobile Human Computer Interaction},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
pages = {30--44},
publisher = {IGI Global},
title = {{A predictive linear regression model for affective state detection of mobile touch screen users}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994051845{\&}doi=10.4018{\%}2FIJMHCI.2017010103{\&}partnerID=40{\&}md5=a3c3eeecad6e2a24b3c416bd99e1d934},
volume = {9},
year = {2017}
}


@article{Bigham2014,
author = {Bigham, Jeffrey P and Bernstein, Michael S},
file = {:Users/changkun/Documents/Mendeley Desktop/2014/Bigham, Bernstein/The handbook of collective intelligence/Bigham, Bernstein - 2014 - Human-Computer Interaction and Collective Intelligence.pdf:pdf},
isbn = {9780262029810},
journal = {The handbook of collective intelligence},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Human-Computer Interaction and Collective Intelligence}},
year = {2014}
}
@article{Hassib2017,
abstract = {Textual communication via mobile phones suffers from a lack of context and emotional awareness. We present a mobile chat application, HeartChat, which integrates heart rate as a cue to increase awareness and empathy. Through a literature review and a focus group, we identified design dimensions important for heart rate augmented chats. We created three concepts showing heart rate per message, in real-time, or sending it ex-plicitly. We tested our system in a two week in-the-wild study with 14 participants (7 pairs). Interviews and questionnaires showed that HeartChat supports empathy between people, in particular close friends and partners. Sharing heart rate helped them to implicitly understand each other's context (e.g. lo-cation, physical activity) and emotional state, and sparked curiosity on special occasions. We discuss opportunities, chal-lenges, and design implications for enriching mobile chats with physiological sensing.},
author = {Hassib, Mariam and Buschek, Daniel and Wozniak, PawelW W and Alt, Florian},
doi = {10.1145/3025453.3025758},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Hassib et al/Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems/Hassib et al. - 2017 - HeartChat Heart Rate Augmented Mobile Chat to Support Empathy and Awareness.pdf:pdf},
isbn = {978-1-4503-4655-9},
journal = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
keywords = {affective computing,heart rate,instant messagingg,physiological sensing},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {2239--2251},
title = {{HeartChat: Heart Rate Augmented Mobile Chat to Support Empathy and Awareness}},
url = {http://doi.acm.org/10.1145/3025453.3025758},
year = {2017}
}
@article{Fragopanagos2005,
abstract = {In this paper, we outline the approach we have developed to construct an emotion-recognising system. It is based on guidance from psychological studies of emotion, as well as from the nature of emotion in its interaction with attention. A neural network architecture is constructed to be able to handle the fusion of different modalities (facial features, prosody and lexical content in speech). Results from the network are given and their implications discussed, as are implications for future direction for the research. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Fragopanagos, N. and Taylor, J. G.},
doi = {10.1016/j.neunet.2005.03.006},
file = {:Users/changkun/Documents/Mendeley Desktop/2005/Fragopanagos, Taylor/Neural Networks/Fragopanagos, Taylor - 2005 - Emotion recognition in human-computer interaction.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Attention control,Emotion classification,Emotion data sets,Emotions,Face feature analysis,Feedback learning,Lexical content,Prosody,Relaxation,Sigma-pi neural networks},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {4},
pages = {389--405},
pmid = {15921887},
title = {{Emotion recognition in human-computer interaction}},
volume = {18},
year = {2005}
}
@article{Lopez-Martinez2017,
abstract = {Pain is a complex and subjective experience that poses a number of measurement challenges. While self-report by the patient is viewed as the gold standard of pain assessment, this approach fails when patients cannot verbally communicate pain intensity or lack normal mental abilities. Here, we present a pain intensity measurement method based on physiological signals. Specifically, we implement a multi-task learning approach based on neural networks that accounts for individual differences in pain responses while still leveraging data from across the population. We test our method in a dataset containing multi-modal physiological responses to nociceptive pain.},
archivePrefix = {arXiv},
arxivId = {1708.08755},
author = {Lopez-Martinez, Daniel and Picard, Rosalind},
eprint = {1708.08755},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Lopez-Martinez, Picard/Unknown/Lopez-Martinez, Picard - 2017 - Multi-task Neural Networks for Personalized Pain Recognition from Physiological Signals.pdf:pdf},
isbn = {9781538606803},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {3--6},
title = {{Multi-task Neural Networks for Personalized Pain Recognition from Physiological Signals}},
url = {http://arxiv.org/abs/1708.08755},
year = {2017}
}
@article{Hihn2016,
abstract = {Humans rely on body gestures and posture when communicating. This topic has been covered in great detail by researchers from various fields. Within this work, approaches to transfer findings in psychology and behavioral studies regarding the relation between gestures and emotions to machine learning methods, will be investigated. Knowledge about the users emotional state is important to achieve human like, natural HCI in modern technical systems. The main focus lies on discriminating between mental overload and mental underload, when completing a given task, which for instance can be useful in an e-tutorial system. Mental underload is a new term used to describe the state a person is in when completing a dull or boring task. A suggestion how the affective states of overload and underload can be expressed using the established notation in the Valence, Arousal and Dominance (V; A;D) space will be given. In a further step it will be shown how to select suitable features, such as gestures, movement and postural behavior patterns. Based on the features selected, a classifier is designed and trained capable of deciding whether a person is experiencing mental overload or underload.},
author = {Hihn, Heinke and Meudt, Sascha and Schwenker, Friedhelm},
doi = {10.1145/3009960.3009961},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Hihn, Meudt, Schwenker/Proceedings of the 2nd workshop on Emotion Representations and Modelling for Companion Systems - ERM4CT '16/Hihn, Meudt, Schwenker - 2016 - Inferring mental overload based on postural behavior and gestures.pdf:pdf},
isbn = {9781450345583},
journal = {Proceedings of the 2nd workshop on Emotion Representations and Modelling for Companion Systems - ERM4CT '16},
keywords = {affective computing,emotion recognition,ensemble meth-},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {1--4},
title = {{Inferring mental overload based on postural behavior and gestures}},
url = {http://dl.acm.org/citation.cfm?doid=3009960.3009961},
year = {2016}
}
@article{Melcer2016,
abstract = {In this paper, we present a study examining how individuals embody emotion within form. Our findings provide a general taxonomy of affective dimensions of shape consistent with and extending previous literature. We also show that ordinary people can reasonably construct embodied shapes using affective dimensions, and illustrate that emotion is conveyed through both visual dimensions and tactile manipulations of shape. Participants used three distinct strategies for embodiment of emotion through shape: the look of a shape (visual representation), creation of a shape symbolizing the experience of an intended emotion (metaphor), and by evoking the intended emotion in the creator through affective movements and manipulations during construction (motion). This work ties together and extends understanding around emotion and form in HCI subdomains such as tangible embodied interaction, emotional assessment, and user experience evaluation.},
author = {Melcer, Edward and Isbister, Katherine},
doi = {10.1145/2851581.2892361},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Melcer, Isbister/Late-Breaking Work Designing Interactive Systems/Melcer, Isbister - 2016 - Motion, Emotion, and Form Exploring Affective Dimensions of Shape.pdf:pdf},
isbn = {9781450340823},
journal = {Late-Breaking Work: Designing Interactive Systems},
keywords = {Affect,Author Keywords Form,Emotion,HCI),Miscellaneous},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {1430--1437},
title = {{Motion, Emotion, and Form: Exploring Affective Dimensions of Shape}},
url = {http://dx.doi.org/10.1145/2851581.2892361},
year = {2016}
}
@article{McDuff2016,
abstract = {We present a real-time facial expression recognition toolkit that can automatically code the expressions of multiple people simultaneously. The toolkit is available across major mobile and desktop platforms (Android, iOS, Windows). The system is trained on the world's largest dataset of facial expressions and has been optimized to operate on mobile devices and with very few false detections. The toolkit offers the potential for the design of novel interfaces that respond to users' emotional states based on their facial expressions. We present a demonstration application that provides real-time visualization of the expressions captured by the camera.},
author = {McDuff, Daniel and Mahmoud, Abdelrahman and Mavadati, Mohammad and Amr, May and Turcot, Jay and el Kaliouby, Rana},
doi = {10.1145/2851581.2890247},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/McDuff et al/Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA '16/McDuff et al. - 2016 - AFFDEX SDK A Cross-Platform Real-Time Multi-Face Expression Recognition Toolkit.pdf:pdf},
isbn = {9781450340823},
journal = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA '16},
keywords = {Author Keywords Facial expressions,Emotion},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {3723--3726},
title = {{AFFDEX SDK: A Cross-Platform Real-Time Multi-Face Expression Recognition Toolkit}},
url = {https://www.affectiva.com/wp-content/uploads/2017/03/McDuff{\_}2016{\_}Affdex.pdf{\%}0Ahttp://dl.acm.org/citation.cfm?doid=2851581.2890247},
year = {2016}
}
@article{Pike2015,
abstract = {{\textcopyright} 2015 ACM.A users interaction with a film typically involves a One Way Affect (1WA), in which the film being consumed has an affect on the consumer. Recent advances in physiological monitoring technology however has facilitated the notion of a Two Way Affect Loop (2WAL), in which a film piece can be dynamically affected by a consumers physiology or behaviour. This paper outlines an agenda for further investigating 2WAL, setting research questions and the inuence of related research areas.},
author = {Pike, Matthew and Ramchurn, Richard and Wilson, Max L},
doi = {10.1145/2783446.2783595},
file = {:Users/changkun/Documents/Mendeley Desktop/2015/Pike, Ramchurn, Wilson/Proceedings of the 2015 British HCI Conference/Pike, Ramchurn, Wilson - 2015 - Two-way Affect Loops in Multimedia Experiences.pdf:pdf},
isbn = {978-1-4503-3643-7},
journal = {Proceedings of the 2015 British HCI Conference},
keywords = {BCI,EEG,adaptive media,digital arts,physiology},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {117--118},
title = {{Two-way Affect Loops in Multimedia Experiences}},
url = {http://doi.acm.org/10.1145/2783446.2783595},
year = {2015}
}
@article{Pham2017,
abstract = {Understanding a target audience's emotional responses to video advertisements is crucial to stakeholders. However, traditional methods for collecting such information are slow, expensive, and coarse-grained. We propose AttentiveVideo, an intelligent mobile interface with corresponding inference algorithms to monitor and quantify the effects of mobile video advertising. AttentiveVideo employs a combination of implicit photoplethysmography (PPG) sensing and facial expression analysis (FEA) to predict viewers' attention, engagement, and sentiment when watching video advertisements on unmodified smartphones. In a 24-participant study, we found that AttentiveVideo achieved good accuracies on a wide range of emotional measures (the best average accuracy = 73.59{\%}, kappa = 0.46 across 9 metrics). We also found that the PPG sensing channel and the FEA technique are complimentary. While FEA works better for strong emotions (e.g., joy and anger), the PPG channel is more informative for subtle responses or emotions. These findings show the potential for both lowcost collection and deep understanding of emotional responses to mobile video advertisements. {\textcopyright} 2017 ACM.},
author = {Pham, Phuong and Wang, Jingtao},
doi = {10.1145/3025171.3025186},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Pham, Wang/Proceedings of the 22nd International Conference on Intelligent User Interfaces - IUI '17/Pham, Wang - 2017 - Understanding Emotional Responses to Mobile Video Advertisements via Physiological Signal Sensing and Facial Express.pdf:pdf},
isbn = {9781450343480},
journal = {Proceedings of the 22nd International Conference on Intelligent User Interfaces - IUI '17},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {67--78},
title = {{Understanding Emotional Responses to Mobile Video Advertisements via Physiological Signal Sensing and Facial Expression Analysis}},
url = {http://dl.acm.org/citation.cfm?doid=3025171.3025186},
year = {2017}
}
@article{Zeng2004,
abstract = {Perhaps the most fundamental application of affective computing would be Human-Computer Interaction (HCI) in which the computer is able to detect and track the user's affective states, and make corresponding feedback. The human multi-sensor affect system defines the expectation of multimodal affect analyzer. In this paper, we present our efforts toward audio-visual HCI-related affect recognition. With HCI applications in mind, we take into account some special affective states which indicate users' cognitive/motivational states. Facing the fact that a facial expression is influenced by both an affective state and speech content, we apply a smoothing method to extract the information of the affective state from facial features. In our fusion stage, a voting method is applied to combine audio and visual modalities so that the final affect recognition accuracy is greatly improved. We test our bimodal affect recognition approach on 38 subjects with 11 HCI-related affect states. The extensive experimental results show that the average person-dependent affect recognition accuracy is almost 90{\%} for our bimodal fusion. Copyright 2004 ACM.},
author = {Zeng, Zhihong and Tu, Jilin and Liu, Ming and Zhang, Tong and Rizzolo, Nicholas and Zhang, Zhenqiu and Huang, Thomas S. and Roth, Dan and Levinson, Stephen},
doi = {10.1145/1027933.1027958},
file = {:Users/changkun/Documents/Mendeley Desktop/2004/Zeng et al/Proceedings of the 6th international conference on Multimodal interfaces - ICMI '04/Zeng et al. - 2004 - Bimodal HCI-related affect recognition.pdf:pdf},
isbn = {1581139950},
journal = {Proceedings of the 6th international conference on Multimodal interfaces  - ICMI '04},
keywords = {affect recognition,affective,emotion recognition,hci,multimodal human-computer interaction},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {137},
title = {{Bimodal HCI-related affect recognition}},
url = {http://portal.acm.org/citation.cfm?doid=1027933.1027958},
year = {2004}
}
@article{Kim2016,
author = {Kim, Joohee and Lee, Na Hyeon and Bae, Byung-Chull and Cho, Jun Dong},
doi = {10.1145/2908805.2909414},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Kim et al/Proceedings of the 2016 ACM Conference Companion Publication on Designing Interactive Systems - DIS '16 Companion/Kim et al. - 2016 - A Feedback System for the Prevention of Forward Head Posture in Sedentary Work Environments.pdf:pdf},
isbn = {9781450343152},
journal = {Proceedings of the 2016 ACM Conference Companion Publication on Designing Interactive Systems - DIS '16 Companion},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {161--164},
title = {{A Feedback System for the Prevention of Forward Head Posture in Sedentary Work Environments}},
url = {http://dl.acm.org/citation.cfm?doid=2908805.2909414},
year = {2016}
}
@article{Cauchard2016,
abstract = {—Drones are becoming more popular and may soon be ubiquitous. As they enter our everyday environments, it becomes critical to ensure their usability through natural Human-Drone Interaction (HDI). Previous work in Human-Robot Interaction (HRI) shows that adding an emotional component is part of the key to success in robots' acceptability. We believe the adoption of personal drones would also benefit from adding an emotional component. This work defines a range of personality traits and emotional attributes that can be encoded in drones through their flight paths. We present a user study (N=20) and show how well three defined emotional states can be recognized. We draw conclusions on interaction techniques with drones and feedback strategies that use the drone's flight path and speed.},
author = {Cauchard, Jessica R. and Zhai, Kevin Y. and Spadafora, Marco and Landay, James A.},
doi = {10.1109/HRI.2016.7451761},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Cauchard et al/ACMIEEE International Conference on Human-Robot Interaction/Cauchard et al. - 2016 - Emotion encoding in human-drone interaction.pdf:pdf},
isbn = {9781467383707},
issn = {21672148},
journal = {ACM/IEEE International Conference on Human-Robot Interaction},
keywords = {Affective computing,Drone,UAV},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {263--270},
title = {{Emotion encoding in human-drone interaction}},
volume = {2016-April},
year = {2016}
}
@article{Tewell2017,
author = {Tewell, Jordan and Bird, Jon and Buchanan, George R.},
doi = {10.1145/3025453.3025844},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Tewell, Bird, Buchanan/Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI '17/Tewell, Bird, Buchanan - 2017 - The Heat is On A Temperature Display for Conveying Affective Feedback.pdf:pdf},
isbn = {9781450346559},
journal = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI '17},
keywords = {affective computing,thermal feedback,thermal haptics},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {1756--1767},
title = {{The Heat is On: A Temperature Display for Conveying Affective Feedback}},
url = {http://dl.acm.org/citation.cfm?doid=3025453.3025844},
year = {2017}
}
@article{Dermody2016,
abstract = {A multimodal system with real-time feedback for public speaking has been developed. The system has been developed within the paradigm of positive computing which focuses on designing for user wellbeing. To date we have focused on the following determinants of wellbeing – autonomy, self-awareness and stress reduction.},
author = {Dermody, Fiona},
doi = {10.1145/2993148.2997616},
file = {:Users/changkun/Documents/Mendeley Desktop/2016/Dermody/Proceedings of the 18th ACM International Conference on Multimodal Interaction - ICMI 2016/Dermody - 2016 - Multimodal positive computing system for public speaking with real-time feedback.pdf:pdf},
isbn = {9781450345569},
journal = {Proceedings of the 18th ACM International Conference on Multimodal Interaction - ICMI 2016},
keywords = {affective computing,hci,multimodal interfaces,positive computing,public speaking,real-time feedback},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {541--545},
title = {{Multimodal positive computing system for public speaking with real-time feedback}},
url = {http://dl.acm.org/citation.cfm?doid=2993148.2997616},
year = {2016}
}
@article{Naidu2015,
author = {Naidu, Ganapreeta R.},
doi = {10.1145/2818346.2823307},
file = {:Users/changkun/Documents/Mendeley Desktop/2015/Naidu/Proceedings of the 2015 ACM on International Conference on Multimodal Interaction - ICMI '15/Naidu - 2015 - A Computational Model of Culture-Specific Emotion Detection for Artificial Agents in the Learning Domain.pdf:pdf},
isbn = {9781450339124},
journal = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction - ICMI '15},
keywords = {affective computing,computer interaction,culture-specific,hci,human,intelligent agents,learning,multimodal interaction},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {635--639},
title = {{A Computational Model of Culture-Specific Emotion Detection for Artificial Agents in the Learning Domain}},
url = {http://dl.acm.org/citation.cfm?doid=2818346.2823307},
year = {2015}
}




@article{Lentini,
author = {Lentini, Rodrigo C and Ionascu, Beatrice and Eyssel, Friederike A and Copti, Scandar and Eid, Mohamad},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Lentini et al/Unknown/Lentini et al. - 2017 - Authoring Tactile Gestures Case Study for Emotion Stimulation.pdf:pdf},
keywords = {arousal,reactions,tactile stimulation,valence},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{Authoring Tactile Gestures : Case Study for Emotion Stimulation}},
year = {2017}
}
@article{Wunarso2017,
author = {Wunarso, Novita Belinda and Soelistio, Yustinus Eko},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Wunarso, Soelistio/Unknown/Wunarso, Soelistio - 2017 - Towards Indonesian Speech-Emotion Automatic Recognition ( I-SpEAR ).pdf:pdf},
keywords = {3,a method by pan,al,and neutral was done,attempt to recognize three,by,daubechies wavelet,emotional states,et,happy,linear mixed effect,sad,speech-emotion recognition,svm},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {8--11},
title = {{Towards Indonesian Speech-Emotion Automatic Recognition ( I-SpEAR )}},
volume = {2017},
year = {2017}
}
@inbook{Tikadar2017,
abstract = {Human Computer Interaction (HCI) can be made more efficient if the interactive systems are able to respond to the users' emotional state. The foremost task for designing such systems is to recognize the users' emotional state during interaction. Most of the interactive systems, now a days, are being made touch enabled. In this work, we propose a model to recognize the emotional state of the users of touchscreen devices. We propose to compute the affective state of the users from 2D screen gesture using the number of touch events and pressure generated for each event as the only two features. No extra hardware setup is required for the computation. Machine learning technique was used for the classification. Four discriminative models, namely the Na{\"{i}}ve Bayes, K-Nearest Neighbor (KNN), Decision Tree and Support Vector Machine (SVM) were explored, with SVM giving the highest accuracy of 96.75{\%}.},
address = {Cham},
author = {Tikadar, Subrata and Kazipeta, Sharath and Ganji, Chandrakanth and Bhattacharya, Samit},
booktitle = {Human-Computer Interact. - INTERACT 2017 16th IFIP TC 13 Int. Conf. Mumbai, India, Sept. 25--29, 2017, Proceedings, Part I},
doi = {10.1007/978-3-319-67744-6_1},
editor = {Bernhaupt, Regina and Dalvi, Girish and Joshi, Anirudha and {K. Balkrishan}, Devanuj and O'Neill, Jacki and Winckler, Marco},
isbn = {978-3-319-67744-6},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {3--12},
publisher = {Springer International Publishing},
title = {{A Minimalist Approach for Identifying Affective States for Mobile Interaction Design}},
url = {https://doi.org/10.1007/978-3-319-67744-6{\_}1},
year = {2017}
}


































































% ====================================================================================
% start visual aspects
% ====================================================================================

@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Hinton, Geoffrey E},
file = {:Users/changkun/Documents/Mendeley Desktop/2012/Krizhevsky, Hinton/Unknown/Krizhevsky, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {ML/CNN,HCI/Seminar: Emotion Inferring},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Simonyan2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {arXiv:1409.1556v6},
file = {:Users/changkun/Documents/Mendeley Desktop/2015/Simonyan, Zisserman/Unknown/Simonyan, Zisserman - 2015 - VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION.pdf:pdf},
mendeley-groups = {ML/CNN,HCI/Seminar: Emotion Inferring},
pages = {1--14},
title = {{VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION}},
year = {2015}
}
@article{Szegedy2014,
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
file = {:Users/changkun/Documents/Mendeley Desktop/2014/Szegedy et al/Unknown/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:pdf},
mendeley-groups = {ML/CNN,HCI/Seminar: Emotion Inferring},
title = {{Going Deeper with Convolutions}},
year = {2014}
}
@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:Users/changkun/Documents/Mendeley Desktop/2014/Szegedy, Vanhoucke, Shlens/Unknown/Szegedy, Vanhoucke, Shlens - 2014 - Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {08866236},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
doi = {10.1016/j.patrec.2014.01.008},
eprint = {1602.07261},
file = {:Users/changkun/Documents/Mendeley Desktop/Unknown/Szegedy et al/Unknown/Szegedy et al. - Unknown - the Impact of Residual Connections on Learning.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
keywords = {Vision},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {4278--4284},
pmid = {23064159},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
year = {2016}
}
@article{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:Users/changkun/Documents/Mendeley Desktop/Unknown/He, Sun/Unknown/He, Sun - Unknown - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {1664-1078},
journal = {2016 IEEE Conf. Comput. Vis. Pattern Recognit.},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {770--778},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://ieeexplore.ieee.org/document/7780459/},
year = {2016}
}
@article{iandola2014densenet,
  title={Densenet: Implementing efficient convnet descriptor pyramids},
  author={Iandola, Forrest and Moskewicz, Matt and Karayev, Sergey and Girshick, Ross and Darrell, Trevor and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1404.1869},
  year={2014}
}
@inproceedings{sabour2017dynamic,
  title={Dynamic Routing Between Capsules},
  author={Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3857--3867},
  year={2017}
}
@inproceedings{he2015convolutional,
  title={Convolutional neural networks at constrained time cost},
  author={He, Kaiming and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5353--5360},
  year={2015}
}
@article{iandola2016squeezenet,
  title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}
@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}
@article{zhang2017shufflenet,
  title={Shufflenet: An extremely efficient convolutional neural network for mobile devices},
  author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  journal={arXiv preprint arXiv:1707.01083},
  year={2017}
}
@inproceedings{goodfellow2013challenges,
  title={Challenges in representation learning: A report on three machine learning contests},
  author={Goodfellow, Ian J and Erhan, Dumitru and Carrier, Pierre Luc and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong-Hyun and others},
  booktitle={International Conference on Neural Information Processing},
  pages={117--124},
  year={2013},
  organization={Springer}
}
@inproceedings{chen20153d,
  title={3D model-based continuous emotion recognition},
  author={Chen, Hui and Li, Jiangdong and Zhang, Fengjun and Li, Yang and Wang, Hongan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1836--1845},
  year={2015}
}
@inproceedings{zhang2016emotion,
  title={Emotion detection using Kinect 3D facial points},
  author={Zhang, Zhan and Cui, Liqing and Liu, Xiaoqian and Zhu, Tingshao},
  booktitle={Web Intelligence (WI), 2016 IEEE/WIC/ACM International Conference on},
  pages={407--410},
  year={2016},
  organization={IEEE}
}
@inproceedings{fabian2016emotionet,
  title={Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild},
  author={Fabian Benitez-Quiroz, C and Srinivasan, Ramprakash and Martinez, Aleix M},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5562--5570},
  year={2016}
}
@inproceedings{mollahosseini2016facial,
  title={Facial expression recognition from world wild web},
  author={Mollahosseini, Ali and Hasani, Behzad and Salvador, Michelle J and Abdollahi, Hojjat and Chan, David and Mahoor, Mohammad H},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={58--65},
  year={2016}
}
@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={779--788},
  year={2016}
}
@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}
@article{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv preprint arXiv:1703.06870},
  year={2017}
}

% ====================================================================================
% end visual aspects
% ====================================================================================
























































% ====================================================================================
% start applications
% ====================================================================================
@inproceedings{langley1997machine,
  title={Machine learning for adaptive user interfaces},
  author={Langley, Pat},
  booktitle={KI-97: Advances in artificial intelligence},
  pages={53--62},
  year={1997},
  organization={Springer}
}
@article{Conati2005,
author = {Conati, Cristina and Marsella, Stacy and Paiva, Ana},
doi = {10.1145/1040830.1040838},
file = {:Users/changkun/Documents/Mendeley Desktop/2005/Conati, Marsella, Paiva/Proceedings of the 10th international conference on Intelligent user interfaces - IUI '05/Conati, Marsella, Paiva - 2005 - Affective interactions.pdf:pdf},
isbn = {1581138946},
journal = {Proc. 10th Int. Conf. Intell. user interfaces  - IUI '05},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {7},
title = {{Affective interactions}},
url = {http://portal.acm.org/citation.cfm?doid=1040830.1040838},
year = {2005}
}
@article{Crane2007,
abstract = {Emotion is a topic of growing interest in the HCI community. Studying emotion within the HCI discipline is an exciting interdisciplinary task. This can be facilitated by the exchange of thoughts and ideas with others working on related projects. The aim of this SIG is to bring together an interdisciplinary group of researchers and practitioners actively working on projects where emotion is an essential component. The goals of the SIG are to identify current themes related to emotion specific HCI work and discuss strategies for moving forward.},
author = {Crane, Elizabeth a and Shami, N Sadat and Peter, Christian},
doi = {10.1145/1240866.1240958},
file = {:Users/changkun/Documents/Mendeley Desktop/2007/Crane, Shami, Peter/Proceedings of ACM CHI 2007 Conference on Human Factors in Computing Systems/Crane, Shami, Peter - 2007 - Let's get emotional emotion research in human computer interaction.pdf:pdf},
isbn = {9781595936424},
journal = {Proceedings of ACM CHI 2007 Conference on Human Factors in Computing Systems},
keywords = {acm classification keywords,affective applications,affective computing,emotion,emotion detection,hci},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {2101--2104},
title = {{Let's get emotional: emotion research in human computer interaction}},
url = {http://doi.acm.org/10.1145/1240866.1240958},
volume = {2},
year = {2007}
}

@misc{surace2001voice,
  title={Voice user interface with personality},
  author={Surace, Kevin J and White, George M and Reeves, Byron B and Nass, Clifford I and Campbell, Mark D and Albert, Roy D and Giangola, James P},
  year={2001},
  month=dec # "~25",
  publisher={Google Patents},
  note={US Patent 6,334,103}
}
@inproceedings{kostov2000emotion,
  title={Emotion in user interface, voice interaction system},
  author={Kostov, V and Fukuda, S},
  booktitle={Systems, Man, and Cybernetics, 2000 IEEE International Conference on},
  volume={2},
  pages={798--803},
  year={2000},
  organization={IEEE}
}
@misc{chang2016intelligent,
  title={Intelligent caring user interface},
  author={Chang, Woosuk and Lang, Angel Camille and Nobumori, Miki and Rigazio, Luca and Senay, Gregory and Sugiura, Akihiko},
  year={2016},
  month=mar # "~30",
  publisher={Google Patents},
  note={US Patent App. 15/085,761}
}
@article{kim2016effects,
  title={Effects of user experience on user resistance to change to the voice user interface of an in-vehicle infotainment system: Implications for platform and standards competition},
  author={Kim, Dong-hyu and Lee, Heejin},
  journal={International Journal of Information Management},
  volume={36},
  number={4},
  pages={653--667},
  year={2016},
  publisher={Elsevier}
}
@inproceedings{mctear2016rise,
  title={The Rise of the Conversational Interface: A New Kid on the Block?},
  author={McTear, Michael F},
  booktitle={International Workshop on Future and Emerging Trends in Language Technology},
  pages={38--49},
  year={2016},
  organization={Springer}
}

@book{schneider1993adaptive,
  title={Adaptive user interfaces: Principles and practice},
  author={Schneider-Hufschmidt, Matthias and Malinowski, Uwe and Kuhme, Thomas},
  year={1993},
  publisher={Elsevier Science Inc.}
}
@article{Dalvand,
author = {Dalvand, Kasra},
file = {:Users/changkun/Documents/Mendeley Desktop/Unknown/Dalvand/Unknown/Dalvand - Unknown - An Adaptive User-Interface Based on User ' s Emotion.pdf:pdf},
keywords = {-human},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
title = {{An Adaptive User-Interface Based on User ' s Emotion}}
}

@article{Kaiser2006,
author = {Kaiser, Robin and Oertel, Karina},
file = {:Users/changkun/Documents/Mendeley Desktop/2006/Kaiser, Oertel/Computer/Kaiser, Oertel - 2006 - Emotions in HCI – An Affective E-Learning System.pdf:pdf},
journal = {Computer (Long. Beach. Calif).},
keywords = {affective computing,are sensors for e,e-learning,evaluated and enhanced,g,hci,integrated in the glove,resistance and skin conductivity,sensor data are wirelessly,skin,transmitted and made available},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
pages = {105--106},
title = {{Emotions in HCI – An Affective E-Learning System}},
year = {2006}
}
@article{Galindo,
author = {Galindo, Juli{\'{a}}n Andr{\'{e}}s and Dupuy-chessa, Sophie and C{\'{e}}ret, {\'{E}}ric and Alpes, Universit{\'{e}} Grenoble and Imag, B{\^{a}}timent and Universitaire, Domaine and Grenoble, F-},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Galindo et al/Unknown/Galindo et al. - 2017 - Toward a User Interface Adaptation Approach Driven by User Emotions.pdf:pdf},
isbn = {9781612085388},
keywords = {-user interface adaptation,architecture,emotion recognition},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {c},
pages = {12--17},
title = {{Toward a User Interface Adaptation Approach Driven by User Emotions}},
year = {2017}
}


% ====================================================================================
% end applications
% ====================================================================================
















































% ====================================================================================
% start challenges
% ====================================================================================
@book{parkinson1995ideas,
  title={Ideas and realities of emotion},
  author={Parkinson, Brian},
  year={1995},
  publisher={Psychology Press}
}
@article{picard2003affective,
  title={Affective computing: challenges},
  author={Picard, Rosalind W},
  journal={International Journal of Human-Computer Studies},
  volume={59},
  number={1},
  pages={55--64},
  year={2003},
  publisher={Elsevier}
}
@article{picard2010affective,
  title={Affective computing: from laughter to IEEE},
  author={Picard, Rosalind W},
  journal={IEEE Transactions on Affective Computing},
  volume={1},
  number={1},
  pages={11--17},
  year={2010},
  publisher={IEEE}
}
@article{cowie2015ethical,
  title={Ethical issues in affective computing},
  author={Cowie, Roddy},
  journal={The Oxford Handbook of Affective Computing},
  pages={334},
  year={2015},
  publisher={Oxford Library of Psychology}
}
@article{rana2015opportunistic,
  title={Opportunistic and context-aware affect sensing on smartphones: the concept, challenges and opportunities},
  author={Rana, Rajib and Hume, Margee and Reilly, John and Jurdak, Raja and Soar, Jeffrey},
  journal={arXiv preprint arXiv:1502.02796},
  year={2015}
}
@article{Zhang2014,
abstract = {This survey presents recent progress on Affective Computing (AC) using mobile devices. AC has been one of the most active research topics for decades. The primary limitation of traditional AC research refers to as impermeable emotions. This criticism is prominent when emotions are investigated outside social contexts. It is problematic because some emotions are directed at other people and arise from interactions with them. The development of smart mobile wearable devices (e.g., Apple Watch, Google Glass, iPhone, Fitbit) enables the wild and natural study for AC in the aspect of computer science. This survey emphasizes the AC study and system using smart wearable devices. Various models, methodologies and systems are discussed in order to examine the state of the art. Finally, we discuss remaining challenges and future works.},
archivePrefix = {arXiv},
arxivId = {1410.1648},
author = {Zhang, Shengkai and Hui, Pan},
eprint = {1410.1648},
file = {:Users/changkun/Documents/Mendeley Desktop/2014/Zhang, Hui/Unknown/Zhang, Hui - 2014 - A Survey on Mobile Affective Computing.pdf:pdf},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
number = {1},
title = {{A Survey on Mobile Affective Computing}},
url = {http://arxiv.org/abs/1410.1648},
year = {2014}
}
@article{Politou2017,
abstract = {The spontaneous recognition of emotional states and personality traits of individuals has been puzzling researchers for years whereas pertinent studies demonstrating the progress in the field, despite their diversity, are still encouraging. This work surveys the most well-known research studies and the state-of-the-art on affect recognition domain based on smartphone acquired data, namely smartphone embedded sensors and smartphone usage. Inevitably, supplementary modalities employed in many eminent studies are also reported here for the sake of completeness. Nevertheless, the intention of the survey is threefold; firstly to document all the to-date relevant literature on affect recognition through smartphone modalities, secondly to argue for the full potential of smartphone use in the inference of affect, and thirdly to demonstrate the current research trends towards mobile affective computing.},
author = {Politou, Eugenia and Alepis, Efthimios and Patsakis, Constantinos},
doi = {10.1016/j.cosrev.2017.07.002},
file = {:Users/changkun/Documents/Mendeley Desktop/2017/Politou, Alepis, Patsakis/Computer Science Review/Politou, Alepis, Patsakis - 2017 - A survey on mobile affective computing.pdf:pdf},
issn = {15740137},
journal = {Comput. Sci. Rev.},
keywords = {Affect detection,Affect recognition,Mobile affective computing,Mobile sensing,Smartphone sensors},
mendeley-groups = {HCI/Seminar: Emotion Inferring},
publisher = {Elsevier Inc.},
title = {{A survey on mobile affective computing}},
url = {http://dx.doi.org/10.1016/j.cosrev.2017.07.002},
year = {2017}
}
@article{ganti2011mobile,
  title={Mobile crowdsensing: current state and future challenges},
  author={Ganti, Raghu K and Ye, Fan and Lei, Hui},
  journal={IEEE Communications Magazine},
  volume={49},
  number={11},
  year={2011},
  publisher={IEEE}
}
@article{khan2013mobile,
  title={Mobile phone sensing systems: A survey},
  author={Khan, Wazir Zada and Xiang, Yang and Aalsalem, Mohammed Y and Arshad, Quratulain},
  journal={IEEE Communications Surveys \& Tutorials},
  volume={15},
  number={1},
  pages={402--427},
  year={2013},
  publisher={IEEE}
}
@article{masuda2008placing,
  title={Placing the face in context: cultural differences in the perception of facial emotion.},
  author={Masuda, Takahiko and Ellsworth, Phoebe C and Mesquita, Batja and Leu, Janxin and Tanida, Shigehito and Van de Veerdonk, Ellen},
  journal={Journal of personality and social psychology},
  volume={94},
  number={3},
  pages={365},
  year={2008},
  publisher={American Psychological Association}
}
@article{gendron2014perceptions,
  title={Perceptions of emotion from facial expressions are not culturally universal: evidence from a remote culture.},
  author={Gendron, Maria and Roberson, Debi and van der Vyver, Jacoba Marietta and Barrett, Lisa Feldman},
  journal={Emotion},
  volume={14},
  number={2},
  pages={251},
  year={2014},
  publisher={American Psychological Association}
}
@article{mesquita1992cultural,
  title={Cultural variations in emotions: a review.},
  author={Mesquita, Batja and Frijda, Nico H},
  journal={Psychological bulletin},
  volume={112},
  number={2},
  pages={179},
  year={1992},
  publisher={American Psychological Association}
}
% ====================================================================================
% end challenges
% ====================================================================================